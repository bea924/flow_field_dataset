{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "base_path = '../../'\n",
    "sys.path.append(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 samples from '/nfs/homedirs/peo/flow_field_dataset/examples/meshgraphnet/datasets/pyvista'.\n",
      "Loaded 100 samples from 'datasets/pyvista'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=34829, num_edges=278612,\n",
       "      ndata_schemes={'BodyID': Scheme(shape=(), dtype=torch.int32), 'SurfaceType': Scheme(shape=(), dtype=torch.int32), 'CellArea': Scheme(shape=(), dtype=torch.float32), 'Normal': Scheme(shape=(3,), dtype=torch.float32), 'ShearStress': Scheme(shape=(3,), dtype=torch.float32), 'Position': Scheme(shape=(3,), dtype=torch.float32), 'Temperature': Scheme(shape=(), dtype=torch.float32), 'Pressure': Scheme(shape=(), dtype=torch.float32)}\n",
       "      edata_schemes={'dx': Scheme(shape=(3,), dtype=torch.float32)})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cooldata.dgl_flow_field_dataset import DGLSurfaceFlowFieldDataset\n",
    "from cooldata.pyvista_flow_field_dataset import PyvistaFlowFieldDataset\n",
    "ds_pv = PyvistaFlowFieldDataset.load_from_huggingface(\"datasets/pyvista\",num_samples=100)\n",
    "ds_dgl = DGLSurfaceFlowFieldDataset(os.path.join(base_path,'datasets/dgl_surface'),ds_pv)\n",
    "ds_dgl = DGLSurfaceFlowFieldDataset(os.path.join(base_path,'datasets/dgl_surface'))\n",
    "ds_dgl[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1158)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dgl[0].ndata[\"Pressure\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[33m2025-06-08 13:56:43.547 (  12.288s) [    79AA50EFE740]      vtkCGNSReader.cxx:4267  WARN| vtkCGNSReader (0x57ab78669770): Skipping BC_t node: BC_t type 'BCInflow' not supported yet.\u001b[0m\n",
      "\u001b[0m\u001b[33m2025-06-08 13:56:43.547 (  12.288s) [    79AA50EFE740]      vtkCGNSReader.cxx:4267  WARN| vtkCGNSReader (0x57ab78669770): Skipping BC_t node: BC_t type 'BCSymmetryPlane' not supported yet.\u001b[0m\n",
      "\u001b[0m\u001b[33m2025-06-08 13:56:43.547 (  12.288s) [    79AA50EFE740]      vtkCGNSReader.cxx:4267  WARN| vtkCGNSReader (0x57ab78669770): Skipping BC_t node: BC_t type 'BCTunnelOutflow' not supported yet.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Arial, sans-serif; margin: 10px; padding: 10px; border: 1px solid #e0e0e0; border-radius: 5px; background-color: #fdfdfd;\">\n",
       "  <h4 style=\"margin-top:0; margin-bottom: 10px; color: #333;\">Design ID: 125002</h4>\n",
       "  <svg width=\"300\" height=\"200\" viewBox=\"-0.02500000000000001 -0.02500000000000001 0.5500000000000002 0.14999999999999997\" style=\"border:1px solid #ccc; background-color: #ffffff;\">\n",
       "    <defs>\n",
       "      <clipPath id=\"clipPath_PyvistaSample_125002_a5ad50\">\n",
       "        <rect x=\"-4.336808689942018e-19\" y=\"0.0\" width=\"0.5000000000000001\" height=\"0.09999999999999996\" />\n",
       "      </clipPath>\n",
       "    </defs>\n",
       "    <rect x=\"-4.336808689942018e-19\" y=\"0.0\" width=\"0.5000000000000001\" height=\"0.09999999999999996\" fill=\"none\" stroke=\"#bbbbbb\" stroke-width=\"0.0025000000000000005\" stroke-dasharray=\"0.005000000000000001,0.005000000000000001\" />\n",
       "    <g clip-path=\"url(#clipPath_PyvistaSample_125002_a5ad50)\">\n",
       "      <rect x=\"-0.05887523768661474\" y=\"0.03861360938769974\" width=\"0.1542503652893203\" height=\"0.05499632681045156\" fill=\"#1f77b4\" fill-opacity=\"0.6\" stroke=\"#1f77b4\" stroke-opacity=\"0.9\" stroke-width=\"0.0012500000000000002\">\n",
       "        <title>Quader 1\\nTemp: 24.8\\nPos: (0.02, 0.07)\\nSize: (0.15, 0.05)</title>\n",
       "      </rect>\n",
       "      <rect x=\"0.015407776365248468\" y=\"0.05874398841686613\" width=\"0.13550081561748564\" height=\"0.05539239193604516\" fill=\"#ff7f0e\" fill-opacity=\"0.6\" stroke=\"#ff7f0e\" stroke-opacity=\"0.9\" stroke-width=\"0.0012500000000000002\">\n",
       "        <title>Quader 2\\nTemp: 72.1\\nPos: (0.08, 0.09)\\nSize: (0.14, 0.06)</title>\n",
       "      </rect>\n",
       "      <rect x=\"0.17784435799780887\" y=\"0.9952433770733221\" width=\"0.05244352457224315\" height=\"0.009513245853355834\" fill=\"#2ca02c\" fill-opacity=\"0.6\" stroke=\"#2ca02c\" stroke-opacity=\"0.9\" stroke-width=\"0.0012500000000000002\">\n",
       "        <title>Quader 3\\nTemp: 40.7\\nPos: (0.20, 1.00)\\nSize: (0.05, 0.01)</title>\n",
       "      </rect>\n",
       "      <rect x=\"0.08072975460149406\" y=\"0.02020616446736425\" width=\"0.08232690355562347\" height=\"0.012072830018052226\" fill=\"#d62728\" fill-opacity=\"0.6\" stroke=\"#d62728\" stroke-opacity=\"0.9\" stroke-width=\"0.0012500000000000002\">\n",
       "        <title>Quader 4\\nTemp: 29.5\\nPos: (0.12, 0.03)\\nSize: (0.08, 0.01)</title>\n",
       "      </rect>\n",
       "      <circle cx=\"0.06680530419385315\" cy=\"1.0\" r=\"0.00340009194521868\" fill=\"#17becf\" fill-opacity=\"0.6\" stroke=\"#17becf\" stroke-opacity=\"0.9\" stroke-width=\"0.0012500000000000002\">\n",
       "        <title>Cylinder 1\\nTemp: 70.8\\nPos: (0.07, 1.00)\\nRadius: 0.00</title>\n",
       "      </circle>\n",
       "      <circle cx=\"0.11484837821122404\" cy=\"1.0\" r=\"0.009343639447153376\" fill=\"#bcbd22\" fill-opacity=\"0.6\" stroke=\"#bcbd22\" stroke-opacity=\"0.9\" stroke-width=\"0.0012500000000000002\">\n",
       "        <title>Cylinder 2\\nTemp: 79.8\\nPos: (0.11, 1.00)\\nRadius: 0.01</title>\n",
       "      </circle>\n",
       "    </g>\n",
       "  </svg>\n",
       "  <p style=\"font-size: 0.8em; color: #666; margin-top: 8px; margin-bottom: 0;\">Bounding Box (xmin, xmax, ymin, ymax): (-0.00, 0.50, 0.00, 0.10)</p>\n",
       "</div>"
      ],
      "text/plain": [
       "<cooldata.pyvista_flow_field_dataset.PyvistaSample at 0x79a93aa5ad50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_pv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=34829, num_edges=278612,\n",
       "      ndata_schemes={'BodyID': Scheme(shape=(), dtype=torch.int32), 'SurfaceType': Scheme(shape=(), dtype=torch.int32), 'CellArea': Scheme(shape=(), dtype=torch.float32), 'Normal': Scheme(shape=(3,), dtype=torch.float32), 'ShearStress': Scheme(shape=(3,), dtype=torch.float32), 'Position': Scheme(shape=(3,), dtype=torch.float32), 'Temperature': Scheme(shape=(), dtype=torch.float32), 'Pressure': Scheme(shape=(), dtype=torch.float32)}\n",
       "      edata_schemes={'dx': Scheme(shape=(3,), dtype=torch.float32)})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dgl[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.dataloading import GraphDataLoader\n",
    "import torch\n",
    "full_dataloader = GraphDataLoader(ds_dgl, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node X:  tensor([[-1.6381,  1.4734,  0.9944,  ...,  0.0000,  1.0000,  0.0000],\n",
      "        [-1.6247,  1.4734,  0.9944,  ...,  0.0000,  1.0000,  0.0000],\n",
      "        [-1.6381,  1.4734,  0.9644,  ...,  0.0000,  1.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 1.6966, -1.4371, -1.2292,  ...,  1.0000,  0.0000,  0.0000],\n",
      "        [ 1.6966, -1.3783, -1.2591,  ...,  1.0000,  0.0000,  0.0000],\n",
      "        [ 1.6966, -1.4371, -1.2591,  ...,  1.0000,  0.0000,  0.0000]]) torch.Size([34829, 11])\n",
      "Edge X:  tensor([[ 1.1945e+00,  2.8222e-10,  1.7533e-10],\n",
      "        [ 2.2358e-10,  2.8222e-10, -3.6570e-01],\n",
      "        [ 1.1945e+00,  2.8222e-10, -3.6570e-01],\n",
      "        ...,\n",
      "        [ 2.2358e-10,  2.8222e-10,  3.6570e-01],\n",
      "        [ 2.2358e-10,  1.3346e+00,  1.7533e-10],\n",
      "        [-5.9725e-01, -6.6731e-01,  1.7533e-10]]) torch.Size([278612, 3])\n",
      "Node Y:  tensor([[ 0.3951, -0.6498, -0.4775,  0.0042, -0.0406],\n",
      "        [ 0.3617, -0.6492, -0.4775,  0.0042, -0.0406],\n",
      "        [ 0.3920, -0.6502, -0.4775,  0.0042, -0.0406],\n",
      "        ...,\n",
      "        [-0.1684, -0.6475, -0.4775,  0.0042, -0.0406],\n",
      "        [-0.1684, -0.6472, -0.4775,  0.0042, -0.0406],\n",
      "        [-0.1684, -0.6472, -0.4775,  0.0042, -0.0406]]) torch.Size([34829, 5])\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "\n",
    "\n",
    "def get_node_edge_X(graph: dgl.DGLGraph):\n",
    "    node_X = torch.cat([graph.ndata[\"Position\"], graph.ndata[\"Normal\"], torch.nn.functional.one_hot(graph.ndata[\"SurfaceType\"].long(), num_classes=5)],dim=1)\n",
    "    edge_X = torch.cat([graph.edata[\"dx\"]],dim=1)\n",
    "    return node_X, edge_X\n",
    "\n",
    "def get_node_Y(graph: dgl.DGLGraph):\n",
    "    return torch.cat([graph.ndata[\"Pressure\"].unsqueeze(1),graph.ndata[\"Temperature\"].unsqueeze(1),graph.ndata['ShearStress']],dim=1)\n",
    "\n",
    "def set_graph_features(graph: dgl.DGLGraph, node_X, edge_X, node_Y):\n",
    "    graph.ndata[\"Position\"] = node_X[:,:3]\n",
    "    graph.ndata[\"Normal\"] = node_X[:,3:6]\n",
    "    graph.ndata[\"SurfaceType\"] = node_X[:,6:11].argmax(dim=1)\n",
    "    graph.edata[\"dx\"] = edge_X\n",
    "    graph.ndata[\"Pressure\"] = node_Y[:,0]\n",
    "    graph.ndata[\"Temperature\"] = node_Y[:,1]\n",
    "    graph.ndata[\"ShearStress\"] = node_Y[:,2:]\n",
    "g=ds_dgl[0]\n",
    "g_cp=g.clone()\n",
    "ndx, edx = get_node_edge_X(g)\n",
    "ndy = get_node_Y(g)\n",
    "set_graph_features(g_cp, ndx, edx, ndy)\n",
    "assert torch.allclose(g_cp.ndata[\"Position\"], g.ndata[\"Position\"])\n",
    "assert torch.allclose(g_cp.ndata[\"Normal\"], g.ndata[\"Normal\"])\n",
    "assert torch.allclose(g_cp.edata[\"dx\"], g.edata[\"dx\"])\n",
    "assert torch.allclose(g_cp.ndata[\"Pressure\"], g.ndata[\"Pressure\"])\n",
    "assert torch.allclose(g_cp.ndata[\"Temperature\"], g.ndata[\"Temperature\"])\n",
    "assert torch.allclose(g_cp.ndata[\"ShearStress\"], g.ndata[\"ShearStress\"])\n",
    "num_node_features = ndx.shape[1]\n",
    "num_edge_features = edx.shape[1]\n",
    "num_node_labels = ndy.shape[1]\n",
    "print(\"Node X: \",ndx, ndx.shape)\n",
    "print(\"Edge X: \",edx, edx.shape)\n",
    "print(\"Node Y: \",ndy, ndy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modulus.models.meshgraphnet import MeshGraphNet\n",
    "model = MeshGraphNet(\n",
    "    input_dim_nodes=num_node_features,\n",
    "    input_dim_edges=num_edge_features,\n",
    "    output_dim=num_node_labels,\n",
    "    aggregation='sum',\n",
    "    hidden_dim_edge_encoder=64,\n",
    "    hidden_dim_node_encoder=64,\n",
    "    hidden_dim_processor=64,\n",
    "    hidden_dim_node_decoder=64\n",
    ")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model=model.to(device)\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "#from torch.amp import GradScaler\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: 0.99985 ** epoch)\n",
    "#scaler = GradScaler(device=device)\n",
    "scaler = None\n",
    "\n",
    "def compute_loss(batch):\n",
    "    batch = batch.to(device)\n",
    "    node_X, edge_X = get_node_edge_X(batch)\n",
    "    node_Y = get_node_Y(batch)\n",
    "    node_Y_pred = model(node_X,edge_X,batch)\n",
    "    batch_pred_graph = batch.clone()\n",
    "    set_graph_features(batch_pred_graph, node_X, edge_X, node_Y_pred)\n",
    "    agg_force_pred = ds_dgl.compute_aggregate_force(batch_pred_graph)\n",
    "    agg_force = ds_dgl.compute_aggregate_force(batch)\n",
    "    #print('Agg force pred: ',format_vector(agg_force_pred.tolist()),' Agg force: ',format_vector(agg_force.tolist()))\n",
    "    return torch.nn.functional.mse_loss(node_Y_pred,node_Y) + 0.8* torch.nn.functional.mse_loss(agg_force_pred,agg_force)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 samples with highest losses: [7, 33, 34, 64, 71, 80, 87, 88, 91, 96]\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "os.makedirs('figures', exist_ok=True)\n",
    "\n",
    "def plot_losses_by_sample():\n",
    "    losses_by_sample = []\n",
    "    for i, batch in enumerate(full_dataloader):\n",
    "        loss = compute_loss(batch)\n",
    "        losses_by_sample.append(loss.item())\n",
    "    plt.hist(losses_by_sample, bins=50)\n",
    "    plt.xlabel('Loss')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Losses in Dataset')\n",
    "    plt.savefig('figures/loss_frequency_dgl.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    top_loss_indices = sorted(range(len(losses_by_sample)), key=lambda i: losses_by_sample[i], reverse=True)[:10]\n",
    "    print(\"10 samples with highest losses:\", sorted(top_loss_indices))\n",
    "    return losses_by_sample\n",
    "losses_by_sample = plot_losses_by_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22bab469dffa4c7abe4f884a00e81320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:44007/index.html?ui=P_0x79a817f6e610_4&reconnect=auto\" class=\"pyviâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyvista as pv\n",
    "pv.set_jupyter_backend(\"trame\")\n",
    "ds_dgl.plot_surface(ds_dgl[33],\"Pressure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median loss: 0.46474090218544006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_loss = torch.median(torch.tensor(losses_by_sample))\n",
    "print(\"Median loss:\", median_loss.item())\n",
    "indices_without_spikes = [i for i, loss in enumerate(losses_by_sample) if loss < median_loss * 5]\n",
    "len(indices_without_spikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 46)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dgl_usable = ds_dgl.select_subset(indices_without_spikes)\n",
    "num_train_samples = int(len(ds_dgl_usable) * 0.5)\n",
    "num_val_samples = len(ds_dgl_usable) - num_train_samples\n",
    "ds_dgl_usable.shuffle()\n",
    "train_ds = ds_dgl_usable.slice(0, num_train_samples)\n",
    "val_ds = ds_dgl_usable.slice(num_train_samples, num_train_samples + num_val_samples)\n",
    "\n",
    "dataloader = GraphDataLoader(train_ds, batch_size=1, shuffle=True)\n",
    "val_dataloader = GraphDataLoader(val_ds, batch_size=1, shuffle=False)\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 0.628010223640336 val loss: 0.5647622677295104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93m`DistributedManager` not initialized already. Initializing now, but this might lead to unexpected errors\u001b[0m\n",
      "/nfs/homedirs/peo/flow_field_dataset/.venv/lib/python3.11/site-packages/modulus/distributed/manager.py:346: UserWarning: Could not initialize using ENV, SLURM or OPENMPI methods. Assuming this is a single process job\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.48589990155564416 val loss: 0.42121172234739945\n",
      "Epoch 2 train loss: 0.46284702403677835 val loss: 0.38662125324101554\n",
      "Epoch 3 train loss: 0.4627443549533685 val loss: 0.39703813276213146\n",
      "Epoch 4 train loss: 0.43621985216935477 val loss: 0.36459360101624677\n",
      "Epoch 5 train loss: 0.42960855257180003 val loss: 0.35806068178752193\n",
      "Epoch 6 train loss: 0.4285951433910264 val loss: 0.3556598854129729\n",
      "Epoch 7 train loss: 0.41839911349945597 val loss: 0.3494250910437625\n",
      "Epoch 8 train loss: 0.41753836191362803 val loss: 0.35483708702351735\n",
      "Epoch 9 train loss: 0.41463371904359925 val loss: 0.35882546396359155\n",
      "Epoch 10 train loss: 0.4081302661862638 val loss: 0.3465907955137284\n",
      "Epoch 11 train loss: 0.40675975059469544 val loss: 0.3442626308001902\n",
      "Epoch 12 train loss: 0.41877316708366075 val loss: 0.3509779007214567\n",
      "Epoch 13 train loss: 0.3876295974685086 val loss: 0.34044535555269406\n",
      "Epoch 14 train loss: 0.4007519239352809 val loss: 0.33496030565837154\n",
      "Epoch 15 train loss: 0.4130159702565935 val loss: 0.3470572777416395\n",
      "Epoch 16 train loss: 0.40190289475851587 val loss: 0.34600477009687736\n",
      "Epoch 17 train loss: 0.39537181274758443 val loss: 0.34362385486779007\n",
      "Epoch 18 train loss: 0.36837819284862944 val loss: 0.32046856860751693\n",
      "Epoch 19 train loss: 0.4014717956384023 val loss: 0.3511340357363224\n",
      "Epoch 20 train loss: 0.3917208925717407 val loss: 0.33505134324988595\n",
      "Epoch 21 train loss: 0.3628170316418012 val loss: 0.31925736391997855\n",
      "Epoch 22 train loss: 0.36813753263817894 val loss: 0.33394917845726013\n",
      "Epoch 23 train loss: 0.37635434849394694 val loss: 0.3472769573006941\n",
      "Epoch 24 train loss: 0.3624687157571316 val loss: 0.32470098696649075\n",
      "Epoch 25 train loss: 0.3582784382833375 val loss: 0.32390840545944544\n",
      "Epoch 26 train loss: 0.3528920128941536 val loss: 0.32498412992319337\n",
      "Epoch 27 train loss: 0.3593789973192745 val loss: 0.3312327910376632\n",
      "Epoch 28 train loss: 0.3690001442200608 val loss: 0.3285344562128834\n",
      "Epoch 29 train loss: 0.35249007046222686 val loss: 0.32017533796960895\n",
      "Epoch 30 train loss: 0.3516664621730646 val loss: 0.32871311789621477\n",
      "Epoch 31 train loss: 0.35376998856663705 val loss: 0.32320640932606615\n",
      "Epoch 32 train loss: 0.35438020163112216 val loss: 0.3191090699933145\n",
      "Epoch 33 train loss: 0.3692710521320502 val loss: 0.3279394536562588\n",
      "Epoch 34 train loss: 0.39858334900604353 val loss: 0.34935109827505506\n",
      "Epoch 35 train loss: 0.3763188149366114 val loss: 0.33658296598688414\n",
      "Epoch 36 train loss: 0.3678995278974374 val loss: 0.3234446364576402\n",
      "Epoch 37 train loss: 0.38865806063016256 val loss: 0.3359515566540801\n",
      "Epoch 38 train loss: 0.37719717679752246 val loss: 0.33448034865052806\n",
      "Epoch 39 train loss: 0.37550743768612543 val loss: 0.33198922815854137\n",
      "Epoch 40 train loss: 0.3611197226577335 val loss: 0.3247329722280088\n",
      "Epoch 41 train loss: 0.36357376020815635 val loss: 0.34024723829782527\n",
      "Epoch 42 train loss: 0.3542653758492735 val loss: 0.31460488700996275\n",
      "Epoch 43 train loss: 0.3557111302183734 val loss: 0.3190498926231395\n",
      "Epoch 44 train loss: 0.35318288372622597 val loss: 0.32962292983480124\n",
      "Epoch 45 train loss: 0.3637229940957493 val loss: 0.3470845902743547\n",
      "Epoch 46 train loss: 0.35025228220555515 val loss: 0.3260341074479663\n",
      "Epoch 47 train loss: 0.35089256771736677 val loss: 0.33134258878619777\n",
      "Epoch 48 train loss: 0.3478357598185539 val loss: 0.32142371611426707\n",
      "Epoch 49 train loss: 0.343676893826988 val loss: 0.3155415923051212\n",
      "Epoch 50 train loss: 0.34748450476262305 val loss: 0.32117597799262276\n",
      "Epoch 51 train loss: 0.34465754015578165 val loss: 0.3165807834947887\n",
      "Epoch 52 train loss: 0.34728149092859695 val loss: 0.31951797753572464\n",
      "Epoch 53 train loss: 0.3468662056658003 val loss: 0.3267614883573159\n",
      "Epoch 54 train loss: 0.3471398189663887 val loss: 0.3255660972031562\n",
      "Epoch 55 train loss: 0.34916729579369227 val loss: 0.31532764766851196\n",
      "Epoch 56 train loss: 0.35208374203907117 val loss: 0.33298814231934754\n",
      "Epoch 57 train loss: 0.3472267375224166 val loss: 0.3290101143976916\n",
      "Epoch 58 train loss: 0.34860470998618337 val loss: 0.334215252538738\n",
      "Epoch 59 train loss: 0.3421879520846738 val loss: 0.3178656619528066\n",
      "Epoch 60 train loss: 0.3535514660179615 val loss: 0.3205406451192887\n",
      "Epoch 61 train loss: 0.3451810206804011 val loss: 0.31760808226207027\n",
      "Epoch 62 train loss: 0.35991071975893446 val loss: 0.32274816663044953\n",
      "Epoch 63 train loss: 0.3446677238576942 val loss: 0.3187507292174775\n",
      "Epoch 64 train loss: 0.34408487718966274 val loss: 0.3230173775683279\n",
      "Epoch 65 train loss: 0.35275113334258396 val loss: 0.33179218905127567\n",
      "Epoch 66 train loss: 0.3417799667351776 val loss: 0.3169074948553158\n",
      "Epoch 67 train loss: 0.3418443422350619 val loss: 0.32023491216418537\n",
      "Epoch 68 train loss: 0.3501216736932596 val loss: 0.3157222296880639\n",
      "Epoch 69 train loss: 0.3452108849253919 val loss: 0.32145181703178777\n",
      "Epoch 70 train loss: 0.34372173117266763 val loss: 0.32795273383026535\n",
      "Epoch 71 train loss: 0.3451318312022421 val loss: 0.32401549783737765\n",
      "Epoch 72 train loss: 0.3379482349587811 val loss: 0.3173284348424362\n",
      "Epoch 73 train loss: 0.34407580428653295 val loss: 0.3153758129185956\n",
      "Epoch 74 train loss: 0.3447968476348453 val loss: 0.3156716954772887\n",
      "Epoch 75 train loss: 0.3400635100901127 val loss: 0.3139710398795812\n",
      "Epoch 76 train loss: 0.3372540912694401 val loss: 0.31247740044541983\n",
      "Epoch 77 train loss: 0.33777509613169565 val loss: 0.3174128155021564\n",
      "Epoch 78 train loss: 0.3400619082980686 val loss: 0.3184217702921318\n",
      "Epoch 79 train loss: 0.33701987912257514 val loss: 0.3160271838955257\n",
      "Epoch 80 train loss: 0.3430943994886345 val loss: 0.32247042834110884\n",
      "Epoch 81 train loss: 0.34359829011890625 val loss: 0.3171317976615999\n",
      "Epoch 82 train loss: 0.34975094116396377 val loss: 0.3207906945406095\n",
      "Epoch 83 train loss: 0.3517642223172718 val loss: 0.34086970774375874\n",
      "Epoch 84 train loss: 0.346380906055371 val loss: 0.31805560964605084\n",
      "Epoch 85 train loss: 0.3447201511926121 val loss: 0.31616932339966297\n",
      "Epoch 86 train loss: 0.3386007082131174 val loss: 0.3127550136135972\n",
      "Epoch 87 train loss: 0.3346318648921119 val loss: 0.3151381919565408\n",
      "Epoch 88 train loss: 0.3413259755406115 val loss: 0.3236111506171849\n",
      "Epoch 89 train loss: 0.3433977622124884 val loss: 0.3243102408621622\n",
      "Epoch 90 train loss: 0.3444702352086703 val loss: 0.32111238906888856\n",
      "Epoch 91 train loss: 0.3357994979454411 val loss: 0.32043407355313713\n",
      "Epoch 92 train loss: 0.3366441906326347 val loss: 0.32144893373808137\n",
      "Epoch 93 train loss: 0.3363291727999846 val loss: 0.32202375720700493\n",
      "Epoch 94 train loss: 0.3442395703660117 val loss: 0.3206901658812295\n",
      "Epoch 95 train loss: 0.33503009006381035 val loss: 0.3173755513423163\n",
      "Epoch 96 train loss: 0.33695359395609964 val loss: 0.3144627443474272\n",
      "Epoch 97 train loss: 0.3372029842601882 val loss: 0.31513440511796786\n",
      "Epoch 98 train loss: 0.3437940435277091 val loss: 0.32965099414729554\n",
      "Epoch 99 train loss: 0.3452117277516259 val loss: 0.32354676610101823\n",
      "Epoch 100 train loss: 0.33566668356458346 val loss: 0.3168645505995854\n",
      "Epoch 101 train loss: 0.34468093067407607 val loss: 0.3148570888392303\n",
      "Epoch 102 train loss: 0.3395086651874913 val loss: 0.3240660455725763\n",
      "Epoch 103 train loss: 0.33635327509707874 val loss: 0.3137439260988132\n",
      "Epoch 104 train loss: 0.3425982013344765 val loss: 0.3194883417986009\n",
      "Epoch 105 train loss: 0.3384210441675451 val loss: 0.3200093779065039\n",
      "Epoch 106 train loss: 0.33358241410719025 val loss: 0.31777666152819345\n",
      "Epoch 107 train loss: 0.33554700480567085 val loss: 0.3242424234909856\n",
      "Epoch 108 train loss: 0.3486721579399374 val loss: 0.3204064851059862\n",
      "Epoch 109 train loss: 0.34113771931992637 val loss: 0.3168234912597615\n",
      "Epoch 110 train loss: 0.33895704257819387 val loss: 0.3233712738298852\n",
      "Epoch 111 train loss: 0.34145259865456157 val loss: 0.3402830078873945\n",
      "Epoch 112 train loss: 0.33868894833657476 val loss: 0.32218878708131937\n",
      "Epoch 113 train loss: 0.3410671258966128 val loss: 0.3225419752947662\n",
      "Epoch 114 train loss: 0.33607266089982457 val loss: 0.324601855048019\n",
      "Epoch 115 train loss: 0.3360908438762029 val loss: 0.31854546102492703\n",
      "Epoch 116 train loss: 0.3319123513168759 val loss: 0.32037102058529854\n",
      "Epoch 117 train loss: 0.33556220854322116 val loss: 0.32581595477202663\n",
      "Epoch 118 train loss: 0.340104820082585 val loss: 0.32608939788263774\n",
      "Epoch 119 train loss: 0.3321534231305122 val loss: 0.3242056349535351\n",
      "Epoch 120 train loss: 0.3366732261247105 val loss: 0.32805357144578645\n",
      "Epoch 121 train loss: 0.3355751944912804 val loss: 0.33137953864491504\n",
      "Epoch 122 train loss: 0.3338531554573112 val loss: 0.3205145481649948\n",
      "Epoch 123 train loss: 0.33188537425465053 val loss: 0.31814282743827155\n",
      "Epoch 124 train loss: 0.3348192053536574 val loss: 0.32465186091544834\n",
      "Epoch 125 train loss: 0.33261944477756816 val loss: 0.32261714468831604\n",
      "Epoch 126 train loss: 0.34788338599933516 val loss: 0.331719361083663\n",
      "Epoch 127 train loss: 0.32910045774446595 val loss: 0.3222727579591067\n",
      "Epoch 128 train loss: 0.3406606079803573 val loss: 0.33065912840159045\n",
      "Epoch 129 train loss: 0.33213161552945775 val loss: 0.314580172624277\n",
      "Epoch 130 train loss: 0.3305145748787456 val loss: 0.3164039518198241\n",
      "Epoch 131 train loss: 0.3277862494190534 val loss: 0.3185635708758365\n",
      "Epoch 132 train loss: 0.3309815015229914 val loss: 0.3271794264893169\n",
      "Epoch 133 train loss: 0.33499532209502325 val loss: 0.3272178580417581\n",
      "Epoch 134 train loss: 0.331929418279065 val loss: 0.3254381991598917\n",
      "Epoch 135 train loss: 0.32717674730552565 val loss: 0.3208274388604838\n",
      "Epoch 136 train loss: 0.32846159330672686 val loss: 0.32151526852470375\n",
      "Epoch 137 train loss: 0.3318300339910719 val loss: 0.33279266127425694\n",
      "Epoch 138 train loss: 0.33241876512765883 val loss: 0.3337581598240396\n",
      "Epoch 139 train loss: 0.33086525913741854 val loss: 0.32336619083324203\n",
      "Epoch 140 train loss: 0.3276573566099008 val loss: 0.31993051958472835\n",
      "Epoch 141 train loss: 0.3248569219476647 val loss: 0.3172854496894971\n",
      "Epoch 142 train loss: 0.326785559207201 val loss: 0.32465983935348364\n",
      "Epoch 143 train loss: 0.32378762107756404 val loss: 0.3180972109346286\n",
      "Epoch 144 train loss: 0.3227229743368096 val loss: 0.32453381780373014\n",
      "Epoch 145 train loss: 0.3239271115925577 val loss: 0.3161709704638823\n",
      "Epoch 146 train loss: 0.3261055669850773 val loss: 0.3307569864327493\n",
      "Epoch 147 train loss: 0.32769994255569246 val loss: 0.32526312889936176\n",
      "Epoch 148 train loss: 0.3287225225733386 val loss: 0.3337494768202305\n",
      "Epoch 149 train loss: 0.32269717935058806 val loss: 0.32837532914203144\n",
      "Epoch 150 train loss: 0.34094056768549813 val loss: 0.33732445489453233\n",
      "Epoch 151 train loss: 0.33103394243452283 val loss: 0.31287227692487446\n",
      "Epoch 152 train loss: 0.3252052561276489 val loss: 0.3174633720646734\n",
      "Epoch 153 train loss: 0.3258089932302634 val loss: 0.31966360472142696\n",
      "Epoch 154 train loss: 0.3277235319217046 val loss: 0.32486595245807065\n",
      "Epoch 155 train loss: 0.3225806506971518 val loss: 0.317238445515218\n",
      "Epoch 156 train loss: 0.3276966700123416 val loss: 0.311478488471197\n",
      "Epoch 157 train loss: 0.33564115812381107 val loss: 0.34273203851088235\n",
      "Epoch 158 train loss: 0.32294390309188103 val loss: 0.3217598203083743\n",
      "Epoch 159 train loss: 0.3237338882353571 val loss: 0.32470388750991097\n",
      "Epoch 160 train loss: 0.32010459875067077 val loss: 0.30791050956948945\n",
      "Epoch 161 train loss: 0.3291677142182986 val loss: 0.32365748853139253\n",
      "Epoch 162 train loss: 0.3191098225613435 val loss: 0.313021546634643\n",
      "Epoch 163 train loss: 0.32027066532108517 val loss: 0.31220167826699174\n",
      "Epoch 164 train loss: 0.3199433681037691 val loss: 0.3204899007535499\n",
      "Epoch 165 train loss: 0.3138125528064039 val loss: 0.3242712947337524\n",
      "Epoch 166 train loss: 0.3233374670975738 val loss: 0.3109465232523887\n",
      "Epoch 167 train loss: 0.3138320276306735 val loss: 0.31281079449083493\n",
      "Epoch 168 train loss: 0.3108458002408346 val loss: 0.31019033329642337\n",
      "Epoch 169 train loss: 0.32693871425257787 val loss: 0.3182215008722699\n",
      "Epoch 170 train loss: 0.3207357226146592 val loss: 0.31175277260658535\n",
      "Epoch 171 train loss: 0.31250407761997645 val loss: 0.3214218195365823\n",
      "Epoch 172 train loss: 0.31565093621611595 val loss: 0.3213666939864988\n",
      "Epoch 173 train loss: 0.3141720517641968 val loss: 0.3176511569839457\n",
      "Epoch 174 train loss: 0.31041673032773864 val loss: 0.3206170198062192\n",
      "Epoch 175 train loss: 0.3092834732598729 val loss: 0.31656606503478857\n",
      "Epoch 176 train loss: 0.3187017695771323 val loss: 0.31087165795590566\n",
      "Epoch 177 train loss: 0.3194899862011274 val loss: 0.31404181310664053\n",
      "Epoch 178 train loss: 0.31701283653577167 val loss: 0.3195605763922567\n",
      "Epoch 179 train loss: 0.30978612908058695 val loss: 0.32235056534409523\n",
      "Epoch 180 train loss: 0.3089443122347196 val loss: 0.31736543572143366\n",
      "Epoch 181 train loss: 0.318634895566437 val loss: 0.3271932470733705\n",
      "Epoch 182 train loss: 0.30860911276605396 val loss: 0.31384737036474375\n",
      "Epoch 183 train loss: 0.31783916039599314 val loss: 0.33203844993334747\n",
      "Epoch 184 train loss: 0.3046744440992673 val loss: 0.3185654226690531\n",
      "Epoch 185 train loss: 0.315827410419782 val loss: 0.32239470614687255\n",
      "Epoch 186 train loss: 0.3066976494259305 val loss: 0.3185326934022748\n",
      "Epoch 187 train loss: 0.30539750191900467 val loss: 0.32033510417070077\n",
      "Epoch 188 train loss: 0.30597866881224844 val loss: 0.3280716845360787\n",
      "Epoch 189 train loss: 0.3141422929035293 val loss: 0.323165617397298\n",
      "Epoch 190 train loss: 0.3072276777691311 val loss: 0.3260158468847689\n",
      "Epoch 191 train loss: 0.3141283190084828 val loss: 0.3352766083958356\n",
      "Epoch 192 train loss: 0.3043334181110064 val loss: 0.31923081450488255\n",
      "Epoch 193 train loss: 0.3068212650716305 val loss: 0.3222033880327059\n",
      "Epoch 194 train loss: 0.3076281264424324 val loss: 0.32317390349572117\n",
      "Epoch 195 train loss: 0.30786540450321304 val loss: 0.31949522945543996\n",
      "Epoch 196 train loss: 0.30527692586183547 val loss: 0.3165709492952927\n",
      "Epoch 197 train loss: 0.3022333642674817 val loss: 0.3252159310909717\n",
      "Epoch 198 train loss: 0.30662346283594766 val loss: 0.32382264295997826\n",
      "Epoch 199 train loss: 0.3013459652662277 val loss: 0.3269297647411409\n",
      "Epoch 200 train loss: 0.30345818657014106 val loss: 0.32491240699006163\n",
      "Epoch 201 train loss: 0.305016096515788 val loss: 0.3229013495147228\n",
      "Epoch 202 train loss: 0.30494515068001216 val loss: 0.31259158780069457\n",
      "Epoch 203 train loss: 0.31030258569452496 val loss: 0.3116117851079806\n",
      "Epoch 204 train loss: 0.30912192720505927 val loss: 0.33745543917883997\n",
      "Epoch 205 train loss: 0.30476862697137724 val loss: 0.31444132368525735\n",
      "Epoch 206 train loss: 0.3063112957610024 val loss: 0.33464556786677113\n",
      "Epoch 207 train loss: 0.3003770093123118 val loss: 0.3199640783764746\n",
      "Epoch 208 train loss: 0.29671881223718327 val loss: 0.314343914632564\n",
      "Epoch 209 train loss: 0.29713019041551486 val loss: 0.32066044454341347\n",
      "Epoch 210 train loss: 0.30222173432509103 val loss: 0.33350624872938445\n",
      "Epoch 211 train loss: 0.2948593397107389 val loss: 0.31688614278707816\n",
      "Epoch 212 train loss: 0.29685247126552794 val loss: 0.3207372580047535\n",
      "Epoch 213 train loss: 0.29865552220079633 val loss: 0.31791362085420155\n",
      "Epoch 214 train loss: 0.30565558589167063 val loss: 0.3161457337277091\n",
      "Epoch 215 train loss: 0.3048805624246597 val loss: 0.3150617429257735\n",
      "Epoch 216 train loss: 0.2954392213788297 val loss: 0.32181923342463764\n",
      "Epoch 217 train loss: 0.29139246435628996 val loss: 0.31168920692542323\n",
      "Epoch 218 train loss: 0.29094688213533826 val loss: 0.3272541686082664\n",
      "Epoch 219 train loss: 0.29307856402463384 val loss: 0.3176256158267674\n",
      "Epoch 220 train loss: 0.29758812735478085 val loss: 0.3149089188841374\n",
      "Epoch 221 train loss: 0.29438229873776434 val loss: 0.31653910219345405\n",
      "Epoch 222 train loss: 0.29547076788213517 val loss: 0.320931503387249\n",
      "Epoch 223 train loss: 0.3063990116947227 val loss: 0.33920947041200555\n",
      "Epoch 224 train loss: 0.2975668346716298 val loss: 0.32957253167810646\n",
      "Epoch 225 train loss: 0.29829828093449273 val loss: 0.3156646650446498\n",
      "Epoch 226 train loss: 0.28788645971152516 val loss: 0.3215403658864291\n",
      "Epoch 227 train loss: 0.29482723681463135 val loss: 0.3206801927122085\n",
      "Epoch 228 train loss: 0.2902591978510221 val loss: 0.3236350234112014\n",
      "Epoch 229 train loss: 0.2982616573572159 val loss: 0.3262174456663754\n",
      "Epoch 230 train loss: 0.295084937579102 val loss: 0.32680498201238073\n",
      "Epoch 231 train loss: 0.30758057344290946 val loss: 0.33685553219655284\n",
      "Epoch 232 train loss: 0.3427614958335956 val loss: 0.3302976888321016\n",
      "Epoch 233 train loss: 0.3052619881927967 val loss: 0.313303765071475\n",
      "Epoch 234 train loss: 0.31000349902444413 val loss: 0.31073379168367904\n",
      "Epoch 235 train loss: 0.2997224220799075 val loss: 0.3151945977107338\n",
      "Epoch 236 train loss: 0.2950290219651328 val loss: 0.3119862238350122\n",
      "Epoch 237 train loss: 0.3003274617923631 val loss: 0.3305607745828836\n",
      "Epoch 238 train loss: 0.29267139641775025 val loss: 0.31829904991647473\n",
      "Epoch 239 train loss: 0.30239258839024435 val loss: 0.30957134894054866\n",
      "Epoch 240 train loss: 0.28811259128981165 val loss: 0.3201278791479442\n",
      "Epoch 241 train loss: 0.28957361512713964 val loss: 0.3224743045056644\n",
      "Epoch 242 train loss: 0.2911900215678745 val loss: 0.31850110148282157\n",
      "Epoch 243 train loss: 0.2881676166421837 val loss: 0.31672683925084444\n",
      "Epoch 244 train loss: 0.29035657867789266 val loss: 0.32568469092897745\n",
      "Epoch 245 train loss: 0.2923140298989084 val loss: 0.31934057734906673\n",
      "Epoch 246 train loss: 0.2944360290136602 val loss: 0.3343362121478371\n",
      "Epoch 247 train loss: 0.2886297827793492 val loss: 0.32529987184249837\n",
      "Epoch 248 train loss: 0.28696303218603136 val loss: 0.31950123980641365\n",
      "Epoch 249 train loss: 0.28368648977743255 val loss: 0.3298216358475063\n",
      "Epoch 250 train loss: 0.2898309366570579 val loss: 0.3277379416901132\n",
      "Epoch 251 train loss: 0.28765366541014775 val loss: 0.329998555795654\n",
      "Epoch 252 train loss: 0.29413548873530493 val loss: 0.32019254265596037\n",
      "Epoch 253 train loss: 0.2860729271339046 val loss: 0.3210493357771117\n",
      "Epoch 254 train loss: 0.28366188522842195 val loss: 0.3167583777204804\n",
      "Epoch 255 train loss: 0.2885437285734547 val loss: 0.32640064677790454\n",
      "Epoch 256 train loss: 0.28890728818045724 val loss: 0.32651689875385037\n",
      "Epoch 257 train loss: 0.2867815662589338 val loss: 0.31760824763256573\n",
      "Epoch 258 train loss: 0.28562870000799495 val loss: 0.3200458602412887\n",
      "Epoch 259 train loss: 0.29246671067343816 val loss: 0.3218270538617735\n",
      "Epoch 260 train loss: 0.2931719322171476 val loss: 0.3112330925853356\n",
      "Epoch 261 train loss: 0.2868968498375681 val loss: 0.32400858175495395\n",
      "Epoch 262 train loss: 0.28022210109564993 val loss: 0.33288011489355046\n",
      "Epoch 263 train loss: 0.2781400921444098 val loss: 0.32441023073118663\n",
      "Epoch 264 train loss: 0.29988916474911903 val loss: 0.31485524239099544\n",
      "Epoch 265 train loss: 0.283143363147974 val loss: 0.32136881958855235\n",
      "Epoch 266 train loss: 0.2871363511515988 val loss: 0.3362019322812557\n",
      "Epoch 267 train loss: 0.280901808043321 val loss: 0.32016916321995464\n",
      "Epoch 268 train loss: 0.2817815277311537 val loss: 0.32193232943182404\n",
      "Epoch 269 train loss: 0.27805938232276173 val loss: 0.3334788061354471\n",
      "Epoch 270 train loss: 0.293057768791914 val loss: 0.3331969599037067\n",
      "Epoch 271 train loss: 0.27847478340069454 val loss: 0.3196065676115129\n",
      "Epoch 272 train loss: 0.2749877043896251 val loss: 0.3177004318846309\n",
      "Epoch 273 train loss: 0.2883558639221721 val loss: 0.3331370738375446\n",
      "Epoch 274 train loss: 0.31230418880780536 val loss: 0.32000883879221004\n",
      "Epoch 275 train loss: 0.29342729623119035 val loss: 0.31592436620722647\n",
      "Epoch 276 train loss: 0.28696152998341456 val loss: 0.3210083155528359\n",
      "Epoch 277 train loss: 0.294655591994524 val loss: 0.3545995804926623\n",
      "Epoch 278 train loss: 0.2830342479878002 val loss: 0.32993256600330706\n",
      "Epoch 279 train loss: 0.2841768133971426 val loss: 0.3200668812445972\n",
      "Epoch 280 train loss: 0.30794290337297653 val loss: 0.3212968342654083\n",
      "Epoch 281 train loss: 0.2803005550470617 val loss: 0.33302339376962703\n",
      "Epoch 282 train loss: 0.27421922327743636 val loss: 0.32661865619213687\n",
      "Epoch 283 train loss: 0.27769456762406564 val loss: 0.32706988918716495\n",
      "Epoch 284 train loss: 0.2760625660419464 val loss: 0.32539337201286916\n",
      "Epoch 285 train loss: 0.27386239502165055 val loss: 0.3232694357309652\n",
      "Epoch 286 train loss: 0.29059369597170087 val loss: 0.3668320931332267\n",
      "Epoch 287 train loss: 0.28501382826103105 val loss: 0.3234039379850678\n",
      "Epoch 288 train loss: 0.2732644882467058 val loss: 0.324360226240495\n",
      "Epoch 289 train loss: 0.2831091809603903 val loss: 0.33786140932984976\n",
      "Epoch 290 train loss: 0.27638092521164154 val loss: 0.3282612551651571\n",
      "Epoch 291 train loss: 0.27469823244545194 val loss: 0.33142230975563114\n",
      "Epoch 292 train loss: 0.26940428308314746 val loss: 0.32389901627017104\n",
      "Epoch 293 train loss: 0.26875493311219745 val loss: 0.32647479900523374\n",
      "Epoch 294 train loss: 0.26790570095181465 val loss: 0.3343614504227172\n",
      "Epoch 295 train loss: 0.28796808363662824 val loss: 0.33540617933739786\n",
      "Epoch 296 train loss: 0.27590392099486455 val loss: 0.3239665563501742\n",
      "Epoch 297 train loss: 0.27191060334444045 val loss: 0.32709750759860745\n",
      "Epoch 298 train loss: 0.26917951645122634 val loss: 0.3388019207379092\n",
      "Epoch 299 train loss: 0.2686776773797141 val loss: 0.3333628126622542\n",
      "Epoch 300 train loss: 0.2737063622309102 val loss: 0.3318208409068377\n",
      "Epoch 301 train loss: 0.26767238188121056 val loss: 0.3350402284413576\n",
      "Epoch 302 train loss: 0.27178511950704787 val loss: 0.3463190521882928\n",
      "Epoch 303 train loss: 0.27064918238255714 val loss: 0.3458306721371153\n",
      "Epoch 304 train loss: 0.27243084435661635 val loss: 0.34408805916166824\n",
      "Epoch 305 train loss: 0.26688754740688536 val loss: 0.3332182628143093\n",
      "Epoch 306 train loss: 0.2665130509270562 val loss: 0.3298653566642948\n",
      "Epoch 307 train loss: 0.26678156836165323 val loss: 0.3396992272818866\n",
      "Epoch 308 train loss: 0.2733983269996113 val loss: 0.33280895860946696\n",
      "Epoch 309 train loss: 0.28324549512730707 val loss: 0.3349891152718793\n",
      "Epoch 310 train loss: 0.2794449490805467 val loss: 0.3202557420439046\n",
      "Epoch 311 train loss: 0.2631603827906979 val loss: 0.33058302557986713\n",
      "Epoch 312 train loss: 0.26693718110521636 val loss: 0.3320602483075598\n",
      "Epoch 313 train loss: 0.2638768720130126 val loss: 0.3332636679482201\n",
      "Epoch 314 train loss: 0.2630597713092963 val loss: 0.33627177831595356\n",
      "Epoch 315 train loss: 0.26389848813414574 val loss: 0.33960387815275916\n",
      "Epoch 316 train loss: 0.2684213666452302 val loss: 0.33054864544259466\n",
      "Epoch 317 train loss: 0.2885240448845757 val loss: 0.3231331153732279\n",
      "Epoch 318 train loss: 0.2730225464536084 val loss: 0.3329666729854501\n",
      "Epoch 319 train loss: 0.2789949561158816 val loss: 0.34317821280463884\n",
      "Epoch 320 train loss: 0.2658227767381403 val loss: 0.3261072580581126\n",
      "Epoch 321 train loss: 0.2654626821478208 val loss: 0.32795561347966606\n",
      "Epoch 322 train loss: 0.26527932915422653 val loss: 0.3272494662391103\n",
      "Epoch 323 train loss: 0.2682178609073162 val loss: 0.33966165845808777\n",
      "Epoch 324 train loss: 0.2597527363234096 val loss: 0.3279744849904724\n",
      "Epoch 325 train loss: 0.2604438935716947 val loss: 0.33519008018724294\n",
      "Epoch 326 train loss: 0.2615743055111832 val loss: 0.33991206137706403\n",
      "Epoch 327 train loss: 0.2734209828078747 val loss: 0.33561597934559634\n",
      "Epoch 328 train loss: 0.27419833466410637 val loss: 0.33094511553645134\n",
      "Epoch 329 train loss: 0.2707902474535836 val loss: 0.3303132810346458\n",
      "Epoch 330 train loss: 0.2658760233885712 val loss: 0.33546762006438297\n",
      "Epoch 331 train loss: 0.2693867231408755 val loss: 0.32547572870617325\n",
      "Epoch 332 train loss: 0.28752146123184097 val loss: 0.3453713712161002\n",
      "Epoch 333 train loss: 0.2807361521654659 val loss: 0.32585863315540814\n",
      "Epoch 334 train loss: 0.27473990784751046 val loss: 0.34271793755824154\n",
      "Epoch 335 train loss: 0.27030519048372903 val loss: 0.3355832699200381\n",
      "Epoch 336 train loss: 0.2652628122932381 val loss: 0.33524005487561226\n",
      "Epoch 337 train loss: 0.26275976730717554 val loss: 0.33128160578401195\n",
      "Epoch 338 train loss: 0.2647736054327753 val loss: 0.35208611943475576\n",
      "Epoch 339 train loss: 0.26072931844327185 val loss: 0.3343461695248666\n",
      "Epoch 340 train loss: 0.2720641176733706 val loss: 0.3225215960132039\n",
      "Epoch 341 train loss: 0.27530133003989854 val loss: 0.34276295396620815\n",
      "Epoch 342 train loss: 0.2628475526140796 val loss: 0.3326001892919126\n",
      "Epoch 343 train loss: 0.2702517213920752 val loss: 0.3568874344067729\n",
      "Epoch 344 train loss: 0.26427954443626933 val loss: 0.33481503820613673\n",
      "Epoch 345 train loss: 0.2641885575320986 val loss: 0.33483124038447504\n",
      "Epoch 346 train loss: 0.26218272422750794 val loss: 0.33175442497367447\n",
      "Epoch 347 train loss: 0.2626961201429367 val loss: 0.33509785082677135\n",
      "Epoch 348 train loss: 0.2575681906607416 val loss: 0.33461652914791\n",
      "Epoch 349 train loss: 0.2611971065402031 val loss: 0.33805520379025006\n",
      "Epoch 350 train loss: 0.2577981601158778 val loss: 0.33927143753870675\n",
      "Epoch 351 train loss: 0.2538798683219486 val loss: 0.33078414468985534\n",
      "Epoch 352 train loss: 0.25552574743827183 val loss: 0.338328803686992\n",
      "Epoch 353 train loss: 0.25560143035319116 val loss: 0.3427090816523718\n",
      "Epoch 354 train loss: 0.2529183328151703 val loss: 0.3322853402275106\n",
      "Epoch 355 train loss: 0.2539661111931006 val loss: 0.3387587891648645\n",
      "Epoch 356 train loss: 0.25404929088221656 val loss: 0.34396923738329305\n",
      "Epoch 357 train loss: 0.2719944265153673 val loss: 0.33346542950881564\n",
      "Epoch 358 train loss: 0.268770346376631 val loss: 0.3396249832990377\n",
      "Epoch 359 train loss: 0.28479876609312166 val loss: 0.3277873325607051\n",
      "Epoch 360 train loss: 0.2704844312535392 val loss: 0.3412614063076351\n",
      "Epoch 361 train loss: 0.26530530411336156 val loss: 0.3286568550797908\n",
      "Epoch 362 train loss: 0.2615073100560241 val loss: 0.3318476468810569\n",
      "Epoch 363 train loss: 0.2825933464699321 val loss: 0.33281708047117875\n",
      "Epoch 364 train loss: 0.2739879373047087 val loss: 0.3270299364205288\n",
      "Epoch 365 train loss: 0.2652204158405463 val loss: 0.33894526019044546\n",
      "Epoch 366 train loss: 0.26973934628897245 val loss: 0.34712615495790605\n",
      "Epoch 367 train loss: 0.29605379982127084 val loss: 0.35609989775263745\n",
      "Epoch 368 train loss: 0.2711140978667471 val loss: 0.32829050379602803\n",
      "Epoch 369 train loss: 0.27216179594397544 val loss: 0.33255999519125273\n",
      "Epoch 370 train loss: 0.2832488466468122 val loss: 0.3398579918657956\n",
      "Epoch 371 train loss: 0.26385125923487873 val loss: 0.32144813919844833\n",
      "Epoch 372 train loss: 0.263124903705385 val loss: 0.33366210013628006\n",
      "Epoch 373 train loss: 0.2550874541203181 val loss: 0.3352808545788993\n",
      "Epoch 374 train loss: 0.2571251923011409 val loss: 0.3277713535758464\n",
      "Epoch 375 train loss: 0.257707433650891 val loss: 0.3270648454680391\n",
      "Epoch 376 train loss: 0.25642987704939313 val loss: 0.32926621992626914\n",
      "Epoch 377 train loss: 0.2517410751018259 val loss: 0.3294016185985959\n",
      "Epoch 378 train loss: 0.24988313871953222 val loss: 0.3295029979199171\n",
      "Epoch 379 train loss: 0.24996769362025792 val loss: 0.34232107703776465\n",
      "Epoch 380 train loss: 0.24941043249434894 val loss: 0.34114607293968613\n",
      "Epoch 381 train loss: 0.24996021100216442 val loss: 0.3405585440442614\n",
      "Epoch 382 train loss: 0.25531272242466607 val loss: 0.3345300137348797\n",
      "Epoch 383 train loss: 0.26152690773208936 val loss: 0.34217806897409586\n",
      "Epoch 384 train loss: 0.26614499530858465 val loss: 0.34188607084038464\n",
      "Epoch 385 train loss: 0.2760343372821808 val loss: 0.33012843447858875\n",
      "Epoch 386 train loss: 0.2683016492260827 val loss: 0.3363076429004255\n",
      "Epoch 387 train loss: 0.26229501167933145 val loss: 0.3487816883817963\n",
      "Epoch 388 train loss: 0.25221144548720786 val loss: 0.3330527152866125\n",
      "Epoch 389 train loss: 0.2510511888398064 val loss: 0.3369272324540045\n",
      "Epoch 390 train loss: 0.26691365126106475 val loss: 0.35554721967681596\n",
      "Epoch 391 train loss: 0.25539858523342346 val loss: 0.3333762746466243\n",
      "Epoch 392 train loss: 0.2532895342343383 val loss: 0.34104073833188286\n",
      "Epoch 393 train loss: 0.25197719832261406 val loss: 0.3424888968305743\n",
      "Epoch 394 train loss: 0.24870639418562254 val loss: 0.33549851494962757\n",
      "Epoch 395 train loss: 0.24826515383190578 val loss: 0.33006303101454093\n",
      "Epoch 396 train loss: 0.2533694968455368 val loss: 0.33277042720304884\n",
      "Epoch 397 train loss: 0.25055023663573794 val loss: 0.34050237565584807\n",
      "Epoch 398 train loss: 0.24736768611603313 val loss: 0.3439343205612639\n",
      "Epoch 399 train loss: 0.24476973803506957 val loss: 0.33971053086545155\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from modulus.launch.utils import save_checkpoint, load_checkpoint\n",
    "checkpoint_path = 'checkpoints'\n",
    "os.makedirs(checkpoint_path,exist_ok=True)\n",
    "epoch_init = 0 #load_checkpoint(checkpoint_path,model,optimizer,scheduler,scaler,device=device)\n",
    "num_epochs = 400\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def compute_average_loss(dataloader):\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            loss = compute_loss(batch)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "for epoch in range(epoch_init,num_epochs):\n",
    "    train_set_loss = compute_average_loss(dataloader)\n",
    "    val_set_loss = compute_average_loss(val_dataloader)\n",
    "    print(f'Epoch {epoch} train loss: {train_set_loss} val loss: {val_set_loss}')\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        loss = compute_loss(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "    scheduler.step()\n",
    "    if epoch % 10 == 0:\n",
    "        save_checkpoint(checkpoint_path,model,optimizer,scheduler,scaler,epoch)\n",
    "save_checkpoint(checkpoint_path,model,optimizer,scheduler,scaler,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50, 100)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataloader), len(dataloader), len(ds_dgl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f6941058194f31bf07ee301cabb25c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:44915/index.html?ui=P_0x7a28660cc8d0_1&reconnect=auto\" class=\"pyviâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.to(device)\n",
    "g=ds_dgl[0].to(device)\n",
    "g_pred = g.clone().to(device)\n",
    "ndx, edx = get_node_edge_X(g)\n",
    "with torch.no_grad():\n",
    "    y_pred = model(ndx,edx,g)\n",
    "set_graph_features(g_pred, ndx, edx, y_pred)\n",
    "ds_dgl.plot_surface(g_pred,\"Pressure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7843e8ee992457ab420f89cb19974a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:44915/index.html?ui=P_0x7a28b3f0f310_2&reconnect=auto\" class=\"pyviâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_dgl.plot_surface(g,\"Pressure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.6068e-10, -6.7290e-02,  0.0000e+00], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dgl.compute_aggregate_force(g, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0038e-04, -3.2547e-02, -5.1706e-05], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dgl.compute_aggregate_force(g_pred,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
