{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "base_path = '../../'\n",
    "sys.path.append(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 samples from '/nfs/homedirs/peo/flow_field_dataset/examples/meshgraphnet/datasets/pyvista'.\n",
      "Loaded 100 samples from 'datasets/pyvista'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting Pyvista dataset to DGLGraphs (surface): 100%|██████████| 100/100 [04:41<00:00,  2.82s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=34829, num_edges=278612,\n",
       "      ndata_schemes={'BodyID': Scheme(shape=(), dtype=torch.int32), 'SurfaceType': Scheme(shape=(), dtype=torch.int32), 'CellArea': Scheme(shape=(), dtype=torch.float32), 'Normal': Scheme(shape=(3,), dtype=torch.float32), 'ShearStress': Scheme(shape=(3,), dtype=torch.float32), 'Position': Scheme(shape=(3,), dtype=torch.float32), 'Temperature': Scheme(shape=(), dtype=torch.float32), 'Pressure': Scheme(shape=(), dtype=torch.float32)}\n",
       "      edata_schemes={'dx': Scheme(shape=(3,), dtype=torch.float32)})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cooldata.dgl_flow_field_dataset import DGLSurfaceFlowFieldDataset\n",
    "from cooldata.pyvista_flow_field_dataset import PyvistaFlowFieldDataset\n",
    "ds_pv = PyvistaFlowFieldDataset.load_from_huggingface(\"datasets/pyvista\",num_samples=100)\n",
    "ds_dgl = DGLSurfaceFlowFieldDataset(os.path.join(base_path,'datasets/dgl_surface'),ds_pv)\n",
    "ds_dgl = DGLSurfaceFlowFieldDataset(os.path.join(base_path,'datasets/dgl_surface'))\n",
    "ds_dgl[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1158)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dgl[0].ndata[\"Pressure\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[33m2025-06-08 09:55:06.197 ( 296.087s) [    79EF04CB9740]      vtkCGNSReader.cxx:4267  WARN| vtkCGNSReader (0x570220c4c2c0): Skipping BC_t node: BC_t type 'BCInflow' not supported yet.\u001b[0m\n",
      "\u001b[0m\u001b[33m2025-06-08 09:55:06.198 ( 296.088s) [    79EF04CB9740]      vtkCGNSReader.cxx:4267  WARN| vtkCGNSReader (0x570220c4c2c0): Skipping BC_t node: BC_t type 'BCSymmetryPlane' not supported yet.\u001b[0m\n",
      "\u001b[0m\u001b[33m2025-06-08 09:55:06.198 ( 296.088s) [    79EF04CB9740]      vtkCGNSReader.cxx:4267  WARN| vtkCGNSReader (0x570220c4c2c0): Skipping BC_t node: BC_t type 'BCTunnelOutflow' not supported yet.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Arial, sans-serif; margin: 10px; padding: 10px; border: 1px solid #e0e0e0; border-radius: 5px; background-color: #fdfdfd;\">\n",
       "  <h4 style=\"margin-top:0; margin-bottom: 10px; color: #333;\">Design ID: 125002</h4>\n",
       "  <svg width=\"300\" height=\"200\" viewBox=\"-0.02500000000000001 -0.02500000000000001 0.5500000000000002 0.14999999999999997\" style=\"border:1px solid #ccc; background-color: #ffffff;\">\n",
       "    <defs>\n",
       "      <clipPath id=\"clipPath_PyvistaSample_125002_7fe810\">\n",
       "        <rect x=\"-4.336808689942018e-19\" y=\"0.0\" width=\"0.5000000000000001\" height=\"0.09999999999999996\" />\n",
       "      </clipPath>\n",
       "    </defs>\n",
       "    <rect x=\"-4.336808689942018e-19\" y=\"0.0\" width=\"0.5000000000000001\" height=\"0.09999999999999996\" fill=\"none\" stroke=\"#bbbbbb\" stroke-width=\"0.0025000000000000005\" stroke-dasharray=\"0.005000000000000001,0.005000000000000001\" />\n",
       "    <g clip-path=\"url(#clipPath_PyvistaSample_125002_7fe810)\">\n",
       "      <rect x=\"-0.05887523768661474\" y=\"0.03861360938769974\" width=\"0.1542503652893203\" height=\"0.05499632681045156\" fill=\"#1f77b4\" fill-opacity=\"0.6\" stroke=\"#1f77b4\" stroke-opacity=\"0.9\" stroke-width=\"0.0012500000000000002\">\n",
       "        <title>Quader 1\\nTemp: 24.8\\nPos: (0.02, 0.07)\\nSize: (0.15, 0.05)</title>\n",
       "      </rect>\n",
       "      <rect x=\"0.015407776365248468\" y=\"0.05874398841686613\" width=\"0.13550081561748564\" height=\"0.05539239193604516\" fill=\"#ff7f0e\" fill-opacity=\"0.6\" stroke=\"#ff7f0e\" stroke-opacity=\"0.9\" stroke-width=\"0.0012500000000000002\">\n",
       "        <title>Quader 2\\nTemp: 72.1\\nPos: (0.08, 0.09)\\nSize: (0.14, 0.06)</title>\n",
       "      </rect>\n",
       "      <rect x=\"0.17784435799780887\" y=\"0.9952433770733221\" width=\"0.05244352457224315\" height=\"0.009513245853355834\" fill=\"#2ca02c\" fill-opacity=\"0.6\" stroke=\"#2ca02c\" stroke-opacity=\"0.9\" stroke-width=\"0.0012500000000000002\">\n",
       "        <title>Quader 3\\nTemp: 40.7\\nPos: (0.20, 1.00)\\nSize: (0.05, 0.01)</title>\n",
       "      </rect>\n",
       "      <rect x=\"0.08072975460149406\" y=\"0.02020616446736425\" width=\"0.08232690355562347\" height=\"0.012072830018052226\" fill=\"#d62728\" fill-opacity=\"0.6\" stroke=\"#d62728\" stroke-opacity=\"0.9\" stroke-width=\"0.0012500000000000002\">\n",
       "        <title>Quader 4\\nTemp: 29.5\\nPos: (0.12, 0.03)\\nSize: (0.08, 0.01)</title>\n",
       "      </rect>\n",
       "      <circle cx=\"0.06680530419385315\" cy=\"1.0\" r=\"0.00340009194521868\" fill=\"#17becf\" fill-opacity=\"0.6\" stroke=\"#17becf\" stroke-opacity=\"0.9\" stroke-width=\"0.0012500000000000002\">\n",
       "        <title>Cylinder 1\\nTemp: 70.8\\nPos: (0.07, 1.00)\\nRadius: 0.00</title>\n",
       "      </circle>\n",
       "      <circle cx=\"0.11484837821122404\" cy=\"1.0\" r=\"0.009343639447153376\" fill=\"#bcbd22\" fill-opacity=\"0.6\" stroke=\"#bcbd22\" stroke-opacity=\"0.9\" stroke-width=\"0.0012500000000000002\">\n",
       "        <title>Cylinder 2\\nTemp: 79.8\\nPos: (0.11, 1.00)\\nRadius: 0.01</title>\n",
       "      </circle>\n",
       "    </g>\n",
       "  </svg>\n",
       "  <p style=\"font-size: 0.8em; color: #666; margin-top: 8px; margin-bottom: 0;\">Bounding Box (xmin, xmax, ymin, ymax): (-0.00, 0.50, 0.00, 0.10)</p>\n",
       "</div>"
      ],
      "text/plain": [
       "<cooldata.pyvista_flow_field_dataset.PyvistaSample at 0x79edee7fe810>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_pv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=34762, num_edges=278724,\n",
       "      ndata_schemes={'BodyID': Scheme(shape=(), dtype=torch.int32), 'SurfaceType': Scheme(shape=(), dtype=torch.int32), 'CellArea': Scheme(shape=(), dtype=torch.float32), 'Normal': Scheme(shape=(3,), dtype=torch.float32), 'ShearStress': Scheme(shape=(3,), dtype=torch.float32), 'Position': Scheme(shape=(3,), dtype=torch.float32), 'Temperature': Scheme(shape=(), dtype=torch.float32), 'Pressure': Scheme(shape=(), dtype=torch.float32)}\n",
       "      edata_schemes={'dx': Scheme(shape=(3,), dtype=torch.float32)})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dgl[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.dataloading import GraphDataLoader\n",
    "import torch\n",
    "full_dataloader = GraphDataLoader(ds_dgl, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node X:  tensor([[-1.6381,  1.4440,  1.0064,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-1.6376,  1.4440, -1.2711,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-1.6247,  1.4440,  1.0064,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 1.6966, -1.3783, -1.1769,  ...,  1.0000,  0.0000,  0.0000],\n",
      "        [ 1.6966, -1.4371, -0.9722,  ...,  1.0000,  0.0000,  0.0000],\n",
      "        [ 1.6966, -1.4371, -1.1780,  ...,  1.0000,  0.0000,  0.0000]]) torch.Size([34762, 11])\n",
      "Edge X:  tensor([[-5.9726e-01, -1.3346e+00, -1.0971e-01],\n",
      "        [ 1.1945e+00,  2.8222e-10,  1.7533e-10],\n",
      "        [ 2.2358e-10, -1.3346e+00,  1.7533e-10],\n",
      "        ...,\n",
      "        [-5.9725e-01, -6.6668e-01,  2.5118e+00],\n",
      "        [-5.9430e-01, -5.8134e-01, -1.1360e+00],\n",
      "        [-5.9364e-01,  1.2525e-03, -1.1360e+00]]) torch.Size([278724, 3])\n",
      "Node Y:  tensor([[ 0.0610, -0.6501,  0.0255, -0.0030, -0.0406],\n",
      "        [ 0.0866, -0.6502, -0.0351, -0.0031, -0.0406],\n",
      "        [ 0.0544, -0.6501, -0.2287, -0.0118, -0.0406],\n",
      "        ...,\n",
      "        [-0.1684, -0.6496, -0.4775,  0.0042, -0.0406],\n",
      "        [-0.1684, -0.6498, -0.4775,  0.0042, -0.0406],\n",
      "        [-0.1684, -0.6497, -0.4775,  0.0042, -0.0406]]) torch.Size([34762, 5])\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "\n",
    "\n",
    "def get_node_edge_X(graph: dgl.DGLGraph):\n",
    "    node_X = torch.cat([graph.ndata[\"Position\"], graph.ndata[\"Normal\"], torch.nn.functional.one_hot(graph.ndata[\"SurfaceType\"].long(), num_classes=5)],dim=1)\n",
    "    edge_X = torch.cat([graph.edata[\"dx\"]],dim=1)\n",
    "    return node_X, edge_X\n",
    "\n",
    "def get_node_Y(graph: dgl.DGLGraph):\n",
    "    return torch.cat([graph.ndata[\"Pressure\"].unsqueeze(1),graph.ndata[\"Temperature\"].unsqueeze(1),graph.ndata['ShearStress']],dim=1)\n",
    "\n",
    "def set_graph_features(graph: dgl.DGLGraph, node_X, edge_X, node_Y):\n",
    "    graph.ndata[\"Position\"] = node_X[:,:3]\n",
    "    graph.ndata[\"Normal\"] = node_X[:,3:6]\n",
    "    graph.ndata[\"SurfaceType\"] = node_X[:,6:11].argmax(dim=1)\n",
    "    graph.edata[\"dx\"] = edge_X\n",
    "    graph.ndata[\"Pressure\"] = node_Y[:,0]\n",
    "    graph.ndata[\"Temperature\"] = node_Y[:,1]\n",
    "    graph.ndata[\"ShearStress\"] = node_Y[:,2:]\n",
    "g=ds_dgl[0]\n",
    "g_cp=g.clone()\n",
    "ndx, edx = get_node_edge_X(g)\n",
    "ndy = get_node_Y(g)\n",
    "set_graph_features(g_cp, ndx, edx, ndy)\n",
    "assert torch.allclose(g_cp.ndata[\"Position\"], g.ndata[\"Position\"])\n",
    "assert torch.allclose(g_cp.ndata[\"Normal\"], g.ndata[\"Normal\"])\n",
    "assert torch.allclose(g_cp.edata[\"dx\"], g.edata[\"dx\"])\n",
    "assert torch.allclose(g_cp.ndata[\"Pressure\"], g.ndata[\"Pressure\"])\n",
    "assert torch.allclose(g_cp.ndata[\"Temperature\"], g.ndata[\"Temperature\"])\n",
    "assert torch.allclose(g_cp.ndata[\"ShearStress\"], g.ndata[\"ShearStress\"])\n",
    "num_node_features = ndx.shape[1]\n",
    "num_edge_features = edx.shape[1]\n",
    "num_node_labels = ndy.shape[1]\n",
    "print(\"Node X: \",ndx, ndx.shape)\n",
    "print(\"Edge X: \",edx, edx.shape)\n",
    "print(\"Node Y: \",ndy, ndy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modulus.models.meshgraphnet import MeshGraphNet\n",
    "model = MeshGraphNet(\n",
    "    input_dim_nodes=num_node_features,\n",
    "    input_dim_edges=num_edge_features,\n",
    "    output_dim=num_node_labels,\n",
    "    aggregation='sum',\n",
    "    hidden_dim_edge_encoder=64,\n",
    "    hidden_dim_node_encoder=64,\n",
    "    hidden_dim_processor=64,\n",
    "    hidden_dim_node_decoder=64\n",
    ")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model=model.to(device)\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "#from torch.amp import GradScaler\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: 0.99985 ** epoch)\n",
    "#scaler = GradScaler(device=device)\n",
    "scaler = None\n",
    "\n",
    "def compute_loss(batch):\n",
    "    batch = batch.to(device)\n",
    "    node_X, edge_X = get_node_edge_X(batch)\n",
    "    node_Y = get_node_Y(batch)\n",
    "    node_Y_pred = model(node_X,edge_X,batch)\n",
    "    batch_pred_graph = batch.clone()\n",
    "    set_graph_features(batch_pred_graph, node_X, edge_X, node_Y_pred)\n",
    "    agg_force_pred = ds_dgl.compute_aggregate_force(batch_pred_graph)\n",
    "    agg_force = ds_dgl.compute_aggregate_force(batch)\n",
    "    #print('Agg force pred: ',format_vector(agg_force_pred.tolist()),' Agg force: ',format_vector(agg_force.tolist()))\n",
    "    return torch.nn.functional.mse_loss(node_Y_pred,node_Y) + 0.8* torch.nn.functional.mse_loss(agg_force_pred,agg_force)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 samples with highest losses: [8, 25, 26, 33, 45, 49, 59, 78, 80, 96]\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "os.makedirs('figures', exist_ok=True)\n",
    "\n",
    "def plot_losses_by_sample():\n",
    "    losses_by_sample = []\n",
    "    for i, batch in enumerate(full_dataloader):\n",
    "        loss = compute_loss(batch)\n",
    "        losses_by_sample.append(loss.item())\n",
    "    plt.hist(losses_by_sample, bins=50)\n",
    "    plt.xlabel('Loss')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Losses in Dataset')\n",
    "    plt.savefig('figures/loss_frequency_dgl.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    top_loss_indices = sorted(range(len(losses_by_sample)), key=lambda i: losses_by_sample[i], reverse=True)[:10]\n",
    "    print(\"10 samples with highest losses:\", sorted(top_loss_indices))\n",
    "    return losses_by_sample\n",
    "losses_by_sample = plot_losses_by_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median loss: 0.46474090218544006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_loss = torch.median(torch.tensor(losses_by_sample))\n",
    "print(\"Median loss:\", median_loss.item())\n",
    "indices_without_spikes = [i for i, loss in enumerate(losses_by_sample) if loss < median_loss * 5]\n",
    "len(indices_without_spikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 46)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dgl_usable = ds_dgl.select_subset(indices_without_spikes)\n",
    "num_train_samples = int(len(ds_dgl_usable) * 0.5)\n",
    "num_val_samples = len(ds_dgl_usable) - num_train_samples\n",
    "ds_dgl_usable.shuffle()\n",
    "train_ds = ds_dgl_usable.slice(0, num_train_samples)\n",
    "val_ds = ds_dgl_usable.slice(num_train_samples, num_train_samples + num_val_samples)\n",
    "\n",
    "dataloader = GraphDataLoader(train_ds, batch_size=1, shuffle=True)\n",
    "val_dataloader = GraphDataLoader(val_ds, batch_size=1, shuffle=False)\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 0.628010223640336 val loss: 0.5647622677295104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93m`DistributedManager` not initialized already. Initializing now, but this might lead to unexpected errors\u001b[0m\n",
      "/nfs/homedirs/peo/flow_field_dataset/.venv/lib/python3.11/site-packages/modulus/distributed/manager.py:346: UserWarning: Could not initialize using ENV, SLURM or OPENMPI methods. Assuming this is a single process job\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.48589990155564416 val loss: 0.42121172234739945\n",
      "Epoch 2 train loss: 0.46284702403677835 val loss: 0.38662125324101554\n",
      "Epoch 3 train loss: 0.4627443549533685 val loss: 0.39703813276213146\n",
      "Epoch 4 train loss: 0.43621985216935477 val loss: 0.36459360101624677\n",
      "Epoch 5 train loss: 0.42960855257180003 val loss: 0.35806068178752193\n",
      "Epoch 6 train loss: 0.4285951433910264 val loss: 0.3556598854129729\n",
      "Epoch 7 train loss: 0.41839911349945597 val loss: 0.3494250910437625\n",
      "Epoch 8 train loss: 0.41753836191362803 val loss: 0.35483708702351735\n",
      "Epoch 9 train loss: 0.41463371904359925 val loss: 0.35882546396359155\n",
      "Epoch 10 train loss: 0.4081302661862638 val loss: 0.3465907955137284\n",
      "Epoch 11 train loss: 0.40675975059469544 val loss: 0.3442626308001902\n",
      "Epoch 12 train loss: 0.41877316708366075 val loss: 0.3509779007214567\n",
      "Epoch 13 train loss: 0.3876295974685086 val loss: 0.34044535555269406\n",
      "Epoch 14 train loss: 0.4007519239352809 val loss: 0.33496030565837154\n",
      "Epoch 15 train loss: 0.4130159702565935 val loss: 0.3470572777416395\n",
      "Epoch 16 train loss: 0.40190289475851587 val loss: 0.34600477009687736\n",
      "Epoch 17 train loss: 0.39537181274758443 val loss: 0.34362385486779007\n",
      "Epoch 18 train loss: 0.36837819284862944 val loss: 0.32046856860751693\n",
      "Epoch 19 train loss: 0.4014717956384023 val loss: 0.3511340357363224\n",
      "Epoch 20 train loss: 0.3917208925717407 val loss: 0.33505134324988595\n",
      "Epoch 21 train loss: 0.3628170316418012 val loss: 0.31925736391997855\n",
      "Epoch 22 train loss: 0.36813753263817894 val loss: 0.33394917845726013\n",
      "Epoch 23 train loss: 0.37635434849394694 val loss: 0.3472769573006941\n",
      "Epoch 24 train loss: 0.3624687157571316 val loss: 0.32470098696649075\n",
      "Epoch 25 train loss: 0.3582784382833375 val loss: 0.32390840545944544\n",
      "Epoch 26 train loss: 0.3528920128941536 val loss: 0.32498412992319337\n",
      "Epoch 27 train loss: 0.3593789973192745 val loss: 0.3312327910376632\n",
      "Epoch 28 train loss: 0.3690001442200608 val loss: 0.3285344562128834\n",
      "Epoch 29 train loss: 0.35249007046222686 val loss: 0.32017533796960895\n",
      "Epoch 30 train loss: 0.3516664621730646 val loss: 0.32871311789621477\n",
      "Epoch 31 train loss: 0.35376998856663705 val loss: 0.32320640932606615\n",
      "Epoch 32 train loss: 0.35438020163112216 val loss: 0.3191090699933145\n",
      "Epoch 33 train loss: 0.3692710521320502 val loss: 0.3279394536562588\n",
      "Epoch 34 train loss: 0.39858334900604353 val loss: 0.34935109827505506\n",
      "Epoch 35 train loss: 0.3763188149366114 val loss: 0.33658296598688414\n",
      "Epoch 36 train loss: 0.3678995278974374 val loss: 0.3234446364576402\n",
      "Epoch 37 train loss: 0.38865806063016256 val loss: 0.3359515566540801\n",
      "Epoch 38 train loss: 0.37719717679752246 val loss: 0.33448034865052806\n",
      "Epoch 39 train loss: 0.37550743768612543 val loss: 0.33198922815854137\n",
      "Epoch 40 train loss: 0.3611197226577335 val loss: 0.3247329722280088\n",
      "Epoch 41 train loss: 0.36357376020815635 val loss: 0.34024723829782527\n",
      "Epoch 42 train loss: 0.3542653758492735 val loss: 0.31460488700996275\n",
      "Epoch 43 train loss: 0.3557111302183734 val loss: 0.3190498926231395\n",
      "Epoch 44 train loss: 0.35318288372622597 val loss: 0.32962292983480124\n",
      "Epoch 45 train loss: 0.3637229940957493 val loss: 0.3470845902743547\n",
      "Epoch 46 train loss: 0.35025228220555515 val loss: 0.3260341074479663\n",
      "Epoch 47 train loss: 0.35089256771736677 val loss: 0.33134258878619777\n",
      "Epoch 48 train loss: 0.3478357598185539 val loss: 0.32142371611426707\n",
      "Epoch 49 train loss: 0.343676893826988 val loss: 0.3155415923051212\n",
      "Epoch 50 train loss: 0.34748450476262305 val loss: 0.32117597799262276\n",
      "Epoch 51 train loss: 0.34465754015578165 val loss: 0.3165807834947887\n",
      "Epoch 52 train loss: 0.34728149092859695 val loss: 0.31951797753572464\n",
      "Epoch 53 train loss: 0.3468662056658003 val loss: 0.3267614883573159\n",
      "Epoch 54 train loss: 0.3471398189663887 val loss: 0.3255660972031562\n",
      "Epoch 55 train loss: 0.34916729579369227 val loss: 0.31532764766851196\n",
      "Epoch 56 train loss: 0.35208374203907117 val loss: 0.33298814231934754\n",
      "Epoch 57 train loss: 0.3472267375224166 val loss: 0.3290101143976916\n",
      "Epoch 58 train loss: 0.34860470998618337 val loss: 0.334215252538738\n",
      "Epoch 59 train loss: 0.3421879520846738 val loss: 0.3178656619528066\n",
      "Epoch 60 train loss: 0.3535514660179615 val loss: 0.3205406451192887\n",
      "Epoch 61 train loss: 0.3451810206804011 val loss: 0.31760808226207027\n",
      "Epoch 62 train loss: 0.35991071975893446 val loss: 0.32274816663044953\n",
      "Epoch 63 train loss: 0.3446677238576942 val loss: 0.3187507292174775\n",
      "Epoch 64 train loss: 0.34408487718966274 val loss: 0.3230173775683279\n",
      "Epoch 65 train loss: 0.35275113334258396 val loss: 0.33179218905127567\n",
      "Epoch 66 train loss: 0.3417799667351776 val loss: 0.3169074948553158\n",
      "Epoch 67 train loss: 0.3418443422350619 val loss: 0.32023491216418537\n",
      "Epoch 68 train loss: 0.3501216736932596 val loss: 0.3157222296880639\n",
      "Epoch 69 train loss: 0.3452108849253919 val loss: 0.32145181703178777\n",
      "Epoch 70 train loss: 0.34372173117266763 val loss: 0.32795273383026535\n",
      "Epoch 71 train loss: 0.3451318312022421 val loss: 0.32401549783737765\n",
      "Epoch 72 train loss: 0.3379482349587811 val loss: 0.3173284348424362\n",
      "Epoch 73 train loss: 0.34407580428653295 val loss: 0.3153758129185956\n",
      "Epoch 74 train loss: 0.3447968476348453 val loss: 0.3156716954772887\n",
      "Epoch 75 train loss: 0.3400635100901127 val loss: 0.3139710398795812\n",
      "Epoch 76 train loss: 0.3372540912694401 val loss: 0.31247740044541983\n",
      "Epoch 77 train loss: 0.33777509613169565 val loss: 0.3174128155021564\n",
      "Epoch 78 train loss: 0.3400619082980686 val loss: 0.3184217702921318\n",
      "Epoch 79 train loss: 0.33701987912257514 val loss: 0.3160271838955257\n",
      "Epoch 80 train loss: 0.3430943994886345 val loss: 0.32247042834110884\n",
      "Epoch 81 train loss: 0.34359829011890625 val loss: 0.3171317976615999\n",
      "Epoch 82 train loss: 0.34975094116396377 val loss: 0.3207906945406095\n",
      "Epoch 83 train loss: 0.3517642223172718 val loss: 0.34086970774375874\n",
      "Epoch 84 train loss: 0.346380906055371 val loss: 0.31805560964605084\n",
      "Epoch 85 train loss: 0.3447201511926121 val loss: 0.31616932339966297\n",
      "Epoch 86 train loss: 0.3386007082131174 val loss: 0.3127550136135972\n",
      "Epoch 87 train loss: 0.3346318648921119 val loss: 0.3151381919565408\n",
      "Epoch 88 train loss: 0.3413259755406115 val loss: 0.3236111506171849\n",
      "Epoch 89 train loss: 0.3433977622124884 val loss: 0.3243102408621622\n",
      "Epoch 90 train loss: 0.3444702352086703 val loss: 0.32111238906888856\n",
      "Epoch 91 train loss: 0.3357994979454411 val loss: 0.32043407355313713\n",
      "Epoch 92 train loss: 0.3366441906326347 val loss: 0.32144893373808137\n",
      "Epoch 93 train loss: 0.3363291727999846 val loss: 0.32202375720700493\n",
      "Epoch 94 train loss: 0.3442395703660117 val loss: 0.3206901658812295\n",
      "Epoch 95 train loss: 0.33503009006381035 val loss: 0.3173755513423163\n",
      "Epoch 96 train loss: 0.33695359395609964 val loss: 0.3144627443474272\n",
      "Epoch 97 train loss: 0.3372029842601882 val loss: 0.31513440511796786\n",
      "Epoch 98 train loss: 0.3437940435277091 val loss: 0.32965099414729554\n",
      "Epoch 99 train loss: 0.3452117277516259 val loss: 0.32354676610101823\n",
      "Epoch 100 train loss: 0.33566668356458346 val loss: 0.3168645505995854\n",
      "Epoch 101 train loss: 0.34468093067407607 val loss: 0.3148570888392303\n",
      "Epoch 102 train loss: 0.3395086651874913 val loss: 0.3240660455725763\n",
      "Epoch 103 train loss: 0.33635327509707874 val loss: 0.3137439260988132\n",
      "Epoch 104 train loss: 0.3425982013344765 val loss: 0.3194883417986009\n",
      "Epoch 105 train loss: 0.3384210441675451 val loss: 0.3200093779065039\n",
      "Epoch 106 train loss: 0.33358241410719025 val loss: 0.31777666152819345\n",
      "Epoch 107 train loss: 0.33554700480567085 val loss: 0.3242424234909856\n",
      "Epoch 108 train loss: 0.3486721579399374 val loss: 0.3204064851059862\n",
      "Epoch 109 train loss: 0.34113771931992637 val loss: 0.3168234912597615\n",
      "Epoch 110 train loss: 0.33895704257819387 val loss: 0.3233712738298852\n",
      "Epoch 111 train loss: 0.34145259865456157 val loss: 0.3402830078873945\n",
      "Epoch 112 train loss: 0.33868894833657476 val loss: 0.32218878708131937\n",
      "Epoch 113 train loss: 0.3410671258966128 val loss: 0.3225419752947662\n",
      "Epoch 114 train loss: 0.33607266089982457 val loss: 0.324601855048019\n",
      "Epoch 115 train loss: 0.3360908438762029 val loss: 0.31854546102492703\n",
      "Epoch 116 train loss: 0.3319123513168759 val loss: 0.32037102058529854\n",
      "Epoch 117 train loss: 0.33556220854322116 val loss: 0.32581595477202663\n",
      "Epoch 118 train loss: 0.340104820082585 val loss: 0.32608939788263774\n",
      "Epoch 119 train loss: 0.3321534231305122 val loss: 0.3242056349535351\n",
      "Epoch 120 train loss: 0.3366732261247105 val loss: 0.32805357144578645\n",
      "Epoch 121 train loss: 0.3355751944912804 val loss: 0.33137953864491504\n",
      "Epoch 122 train loss: 0.3338531554573112 val loss: 0.3205145481649948\n",
      "Epoch 123 train loss: 0.33188537425465053 val loss: 0.31814282743827155\n",
      "Epoch 124 train loss: 0.3348192053536574 val loss: 0.32465186091544834\n",
      "Epoch 125 train loss: 0.33261944477756816 val loss: 0.32261714468831604\n",
      "Epoch 126 train loss: 0.34788338599933516 val loss: 0.331719361083663\n",
      "Epoch 127 train loss: 0.32910045774446595 val loss: 0.3222727579591067\n",
      "Epoch 128 train loss: 0.3406606079803573 val loss: 0.33065912840159045\n",
      "Epoch 129 train loss: 0.33213161552945775 val loss: 0.314580172624277\n",
      "Epoch 130 train loss: 0.3305145748787456 val loss: 0.3164039518198241\n",
      "Epoch 131 train loss: 0.3277862494190534 val loss: 0.3185635708758365\n",
      "Epoch 132 train loss: 0.3309815015229914 val loss: 0.3271794264893169\n",
      "Epoch 133 train loss: 0.33499532209502325 val loss: 0.3272178580417581\n",
      "Epoch 134 train loss: 0.331929418279065 val loss: 0.3254381991598917\n",
      "Epoch 135 train loss: 0.32717674730552565 val loss: 0.3208274388604838\n",
      "Epoch 136 train loss: 0.32846159330672686 val loss: 0.32151526852470375\n",
      "Epoch 137 train loss: 0.3318300339910719 val loss: 0.33279266127425694\n",
      "Epoch 138 train loss: 0.33241876512765883 val loss: 0.3337581598240396\n",
      "Epoch 139 train loss: 0.33086525913741854 val loss: 0.32336619083324203\n",
      "Epoch 140 train loss: 0.3276573566099008 val loss: 0.31993051958472835\n",
      "Epoch 141 train loss: 0.3248569219476647 val loss: 0.3172854496894971\n",
      "Epoch 142 train loss: 0.326785559207201 val loss: 0.32465983935348364\n",
      "Epoch 143 train loss: 0.32378762107756404 val loss: 0.3180972109346286\n",
      "Epoch 144 train loss: 0.3227229743368096 val loss: 0.32453381780373014\n",
      "Epoch 145 train loss: 0.3239271115925577 val loss: 0.3161709704638823\n",
      "Epoch 146 train loss: 0.3261055669850773 val loss: 0.3307569864327493\n",
      "Epoch 147 train loss: 0.32769994255569246 val loss: 0.32526312889936176\n",
      "Epoch 148 train loss: 0.3287225225733386 val loss: 0.3337494768202305\n",
      "Epoch 149 train loss: 0.32269717935058806 val loss: 0.32837532914203144\n",
      "Epoch 150 train loss: 0.34094056768549813 val loss: 0.33732445489453233\n",
      "Epoch 151 train loss: 0.33103394243452283 val loss: 0.31287227692487446\n",
      "Epoch 152 train loss: 0.3252052561276489 val loss: 0.3174633720646734\n",
      "Epoch 153 train loss: 0.3258089932302634 val loss: 0.31966360472142696\n",
      "Epoch 154 train loss: 0.3277235319217046 val loss: 0.32486595245807065\n",
      "Epoch 155 train loss: 0.3225806506971518 val loss: 0.317238445515218\n",
      "Epoch 156 train loss: 0.3276966700123416 val loss: 0.311478488471197\n",
      "Epoch 157 train loss: 0.33564115812381107 val loss: 0.34273203851088235\n",
      "Epoch 158 train loss: 0.32294390309188103 val loss: 0.3217598203083743\n",
      "Epoch 159 train loss: 0.3237338882353571 val loss: 0.32470388750991097\n",
      "Epoch 160 train loss: 0.32010459875067077 val loss: 0.30791050956948945\n",
      "Epoch 161 train loss: 0.3291677142182986 val loss: 0.32365748853139253\n",
      "Epoch 162 train loss: 0.3191098225613435 val loss: 0.313021546634643\n",
      "Epoch 163 train loss: 0.32027066532108517 val loss: 0.31220167826699174\n",
      "Epoch 164 train loss: 0.3199433681037691 val loss: 0.3204899007535499\n",
      "Epoch 165 train loss: 0.3138125528064039 val loss: 0.3242712947337524\n",
      "Epoch 166 train loss: 0.3233374670975738 val loss: 0.3109465232523887\n",
      "Epoch 167 train loss: 0.3138320276306735 val loss: 0.31281079449083493\n",
      "Epoch 168 train loss: 0.3108458002408346 val loss: 0.31019033329642337\n",
      "Epoch 169 train loss: 0.32693871425257787 val loss: 0.3182215008722699\n",
      "Epoch 170 train loss: 0.3207357226146592 val loss: 0.31175277260658535\n",
      "Epoch 171 train loss: 0.31250407761997645 val loss: 0.3214218195365823\n",
      "Epoch 172 train loss: 0.31565093621611595 val loss: 0.3213666939864988\n",
      "Epoch 173 train loss: 0.3141720517641968 val loss: 0.3176511569839457\n",
      "Epoch 174 train loss: 0.31041673032773864 val loss: 0.3206170198062192\n",
      "Epoch 175 train loss: 0.3092834732598729 val loss: 0.31656606503478857\n",
      "Epoch 176 train loss: 0.3187017695771323 val loss: 0.31087165795590566\n",
      "Epoch 177 train loss: 0.3194899862011274 val loss: 0.31404181310664053\n",
      "Epoch 178 train loss: 0.31701283653577167 val loss: 0.3195605763922567\n",
      "Epoch 179 train loss: 0.30978612908058695 val loss: 0.32235056534409523\n",
      "Epoch 180 train loss: 0.3089443122347196 val loss: 0.31736543572143366\n",
      "Epoch 181 train loss: 0.318634895566437 val loss: 0.3271932470733705\n",
      "Epoch 182 train loss: 0.30860911276605396 val loss: 0.31384737036474375\n",
      "Epoch 183 train loss: 0.31783916039599314 val loss: 0.33203844993334747\n",
      "Epoch 184 train loss: 0.3046744440992673 val loss: 0.3185654226690531\n",
      "Epoch 185 train loss: 0.315827410419782 val loss: 0.32239470614687255\n",
      "Epoch 186 train loss: 0.3066976494259305 val loss: 0.3185326934022748\n",
      "Epoch 187 train loss: 0.30539750191900467 val loss: 0.32033510417070077\n",
      "Epoch 188 train loss: 0.30597866881224844 val loss: 0.3280716845360787\n",
      "Epoch 189 train loss: 0.3141422929035293 val loss: 0.323165617397298\n",
      "Epoch 190 train loss: 0.3072276777691311 val loss: 0.3260158468847689\n",
      "Epoch 191 train loss: 0.3141283190084828 val loss: 0.3352766083958356\n",
      "Epoch 192 train loss: 0.3043334181110064 val loss: 0.31923081450488255\n",
      "Epoch 193 train loss: 0.3068212650716305 val loss: 0.3222033880327059\n",
      "Epoch 194 train loss: 0.3076281264424324 val loss: 0.32317390349572117\n",
      "Epoch 195 train loss: 0.30786540450321304 val loss: 0.31949522945543996\n",
      "Epoch 196 train loss: 0.30527692586183547 val loss: 0.3165709492952927\n",
      "Epoch 197 train loss: 0.3022333642674817 val loss: 0.3252159310909717\n",
      "Epoch 198 train loss: 0.30662346283594766 val loss: 0.32382264295997826\n",
      "Epoch 199 train loss: 0.3013459652662277 val loss: 0.3269297647411409\n",
      "Epoch 200 train loss: 0.30345818657014106 val loss: 0.32491240699006163\n",
      "Epoch 201 train loss: 0.305016096515788 val loss: 0.3229013495147228\n",
      "Epoch 202 train loss: 0.30494515068001216 val loss: 0.31259158780069457\n",
      "Epoch 203 train loss: 0.31030258569452496 val loss: 0.3116117851079806\n",
      "Epoch 204 train loss: 0.30912192720505927 val loss: 0.33745543917883997\n",
      "Epoch 205 train loss: 0.30476862697137724 val loss: 0.31444132368525735\n",
      "Epoch 206 train loss: 0.3063112957610024 val loss: 0.33464556786677113\n",
      "Epoch 207 train loss: 0.3003770093123118 val loss: 0.3199640783764746\n",
      "Epoch 208 train loss: 0.29671881223718327 val loss: 0.314343914632564\n",
      "Epoch 209 train loss: 0.29713019041551486 val loss: 0.32066044454341347\n",
      "Epoch 210 train loss: 0.30222173432509103 val loss: 0.33350624872938445\n",
      "Epoch 211 train loss: 0.2948593397107389 val loss: 0.31688614278707816\n",
      "Epoch 212 train loss: 0.29685247126552794 val loss: 0.3207372580047535\n",
      "Epoch 213 train loss: 0.29865552220079633 val loss: 0.31791362085420155\n",
      "Epoch 214 train loss: 0.30565558589167063 val loss: 0.3161457337277091\n",
      "Epoch 215 train loss: 0.3048805624246597 val loss: 0.3150617429257735\n",
      "Epoch 216 train loss: 0.2954392213788297 val loss: 0.32181923342463764\n",
      "Epoch 217 train loss: 0.29139246435628996 val loss: 0.31168920692542323\n",
      "Epoch 218 train loss: 0.29094688213533826 val loss: 0.3272541686082664\n",
      "Epoch 219 train loss: 0.29307856402463384 val loss: 0.3176256158267674\n",
      "Epoch 220 train loss: 0.29758812735478085 val loss: 0.3149089188841374\n",
      "Epoch 221 train loss: 0.29438229873776434 val loss: 0.31653910219345405\n",
      "Epoch 222 train loss: 0.29547076788213517 val loss: 0.320931503387249\n",
      "Epoch 223 train loss: 0.3063990116947227 val loss: 0.33920947041200555\n",
      "Epoch 224 train loss: 0.2975668346716298 val loss: 0.32957253167810646\n",
      "Epoch 225 train loss: 0.29829828093449273 val loss: 0.3156646650446498\n",
      "Epoch 226 train loss: 0.28788645971152516 val loss: 0.3215403658864291\n",
      "Epoch 227 train loss: 0.29482723681463135 val loss: 0.3206801927122085\n",
      "Epoch 228 train loss: 0.2902591978510221 val loss: 0.3236350234112014\n",
      "Epoch 229 train loss: 0.2982616573572159 val loss: 0.3262174456663754\n",
      "Epoch 230 train loss: 0.295084937579102 val loss: 0.32680498201238073\n",
      "Epoch 231 train loss: 0.30758057344290946 val loss: 0.33685553219655284\n",
      "Epoch 232 train loss: 0.3427614958335956 val loss: 0.3302976888321016\n",
      "Epoch 233 train loss: 0.3052619881927967 val loss: 0.313303765071475\n",
      "Epoch 234 train loss: 0.31000349902444413 val loss: 0.31073379168367904\n",
      "Epoch 235 train loss: 0.2997224220799075 val loss: 0.3151945977107338\n",
      "Epoch 236 train loss: 0.2950290219651328 val loss: 0.3119862238350122\n",
      "Epoch 237 train loss: 0.3003274617923631 val loss: 0.3305607745828836\n",
      "Epoch 238 train loss: 0.29267139641775025 val loss: 0.31829904991647473\n",
      "Epoch 239 train loss: 0.30239258839024435 val loss: 0.30957134894054866\n",
      "Epoch 240 train loss: 0.28811259128981165 val loss: 0.3201278791479442\n",
      "Epoch 241 train loss: 0.28957361512713964 val loss: 0.3224743045056644\n",
      "Epoch 242 train loss: 0.2911900215678745 val loss: 0.31850110148282157\n",
      "Epoch 243 train loss: 0.2881676166421837 val loss: 0.31672683925084444\n",
      "Epoch 244 train loss: 0.29035657867789266 val loss: 0.32568469092897745\n",
      "Epoch 245 train loss: 0.2923140298989084 val loss: 0.31934057734906673\n",
      "Epoch 246 train loss: 0.2944360290136602 val loss: 0.3343362121478371\n",
      "Epoch 247 train loss: 0.2886297827793492 val loss: 0.32529987184249837\n",
      "Epoch 248 train loss: 0.28696303218603136 val loss: 0.31950123980641365\n",
      "Epoch 249 train loss: 0.28368648977743255 val loss: 0.3298216358475063\n",
      "Epoch 250 train loss: 0.2898309366570579 val loss: 0.3277379416901132\n",
      "Epoch 251 train loss: 0.28765366541014775 val loss: 0.329998555795654\n",
      "Epoch 252 train loss: 0.29413548873530493 val loss: 0.32019254265596037\n",
      "Epoch 253 train loss: 0.2860729271339046 val loss: 0.3210493357771117\n",
      "Epoch 254 train loss: 0.28366188522842195 val loss: 0.3167583777204804\n",
      "Epoch 255 train loss: 0.2885437285734547 val loss: 0.32640064677790454\n",
      "Epoch 256 train loss: 0.28890728818045724 val loss: 0.32651689875385037\n",
      "Epoch 257 train loss: 0.2867815662589338 val loss: 0.31760824763256573\n",
      "Epoch 258 train loss: 0.28562870000799495 val loss: 0.3200458602412887\n",
      "Epoch 259 train loss: 0.29246671067343816 val loss: 0.3218270538617735\n",
      "Epoch 260 train loss: 0.2931719322171476 val loss: 0.3112330925853356\n",
      "Epoch 261 train loss: 0.2868968498375681 val loss: 0.32400858175495395\n",
      "Epoch 262 train loss: 0.28022210109564993 val loss: 0.33288011489355046\n",
      "Epoch 263 train loss: 0.2781400921444098 val loss: 0.32441023073118663\n",
      "Epoch 264 train loss: 0.29988916474911903 val loss: 0.31485524239099544\n",
      "Epoch 265 train loss: 0.283143363147974 val loss: 0.32136881958855235\n",
      "Epoch 266 train loss: 0.2871363511515988 val loss: 0.3362019322812557\n",
      "Epoch 267 train loss: 0.280901808043321 val loss: 0.32016916321995464\n",
      "Epoch 268 train loss: 0.2817815277311537 val loss: 0.32193232943182404\n",
      "Epoch 269 train loss: 0.27805938232276173 val loss: 0.3334788061354471\n",
      "Epoch 270 train loss: 0.293057768791914 val loss: 0.3331969599037067\n",
      "Epoch 271 train loss: 0.27847478340069454 val loss: 0.3196065676115129\n",
      "Epoch 272 train loss: 0.2749877043896251 val loss: 0.3177004318846309\n",
      "Epoch 273 train loss: 0.2883558639221721 val loss: 0.3331370738375446\n",
      "Epoch 274 train loss: 0.31230418880780536 val loss: 0.32000883879221004\n",
      "Epoch 275 train loss: 0.29342729623119035 val loss: 0.31592436620722647\n",
      "Epoch 276 train loss: 0.28696152998341456 val loss: 0.3210083155528359\n",
      "Epoch 277 train loss: 0.294655591994524 val loss: 0.3545995804926623\n",
      "Epoch 278 train loss: 0.2830342479878002 val loss: 0.32993256600330706\n",
      "Epoch 279 train loss: 0.2841768133971426 val loss: 0.3200668812445972\n",
      "Epoch 280 train loss: 0.30794290337297653 val loss: 0.3212968342654083\n",
      "Epoch 281 train loss: 0.2803005550470617 val loss: 0.33302339376962703\n",
      "Epoch 282 train loss: 0.27421922327743636 val loss: 0.32661865619213687\n",
      "Epoch 283 train loss: 0.27769456762406564 val loss: 0.32706988918716495\n",
      "Epoch 284 train loss: 0.2760625660419464 val loss: 0.32539337201286916\n",
      "Epoch 285 train loss: 0.27386239502165055 val loss: 0.3232694357309652\n",
      "Epoch 286 train loss: 0.29059369597170087 val loss: 0.3668320931332267\n",
      "Epoch 287 train loss: 0.28501382826103105 val loss: 0.3234039379850678\n",
      "Epoch 288 train loss: 0.2732644882467058 val loss: 0.324360226240495\n",
      "Epoch 289 train loss: 0.2831091809603903 val loss: 0.33786140932984976\n",
      "Epoch 290 train loss: 0.27638092521164154 val loss: 0.3282612551651571\n",
      "Epoch 291 train loss: 0.27469823244545194 val loss: 0.33142230975563114\n",
      "Epoch 292 train loss: 0.26940428308314746 val loss: 0.32389901627017104\n",
      "Epoch 293 train loss: 0.26875493311219745 val loss: 0.32647479900523374\n",
      "Epoch 294 train loss: 0.26790570095181465 val loss: 0.3343614504227172\n",
      "Epoch 295 train loss: 0.28796808363662824 val loss: 0.33540617933739786\n",
      "Epoch 296 train loss: 0.27590392099486455 val loss: 0.3239665563501742\n",
      "Epoch 297 train loss: 0.27191060334444045 val loss: 0.32709750759860745\n",
      "Epoch 298 train loss: 0.26917951645122634 val loss: 0.3388019207379092\n",
      "Epoch 299 train loss: 0.2686776773797141 val loss: 0.3333628126622542\n",
      "Epoch 300 train loss: 0.2737063622309102 val loss: 0.3318208409068377\n",
      "Epoch 301 train loss: 0.26767238188121056 val loss: 0.3350402284413576\n",
      "Epoch 302 train loss: 0.27178511950704787 val loss: 0.3463190521882928\n",
      "Epoch 303 train loss: 0.27064918238255714 val loss: 0.3458306721371153\n",
      "Epoch 304 train loss: 0.27243084435661635 val loss: 0.34408805916166824\n",
      "Epoch 305 train loss: 0.26688754740688536 val loss: 0.3332182628143093\n",
      "Epoch 306 train loss: 0.2665130509270562 val loss: 0.3298653566642948\n",
      "Epoch 307 train loss: 0.26678156836165323 val loss: 0.3396992272818866\n",
      "Epoch 308 train loss: 0.2733983269996113 val loss: 0.33280895860946696\n",
      "Epoch 309 train loss: 0.28324549512730707 val loss: 0.3349891152718793\n",
      "Epoch 310 train loss: 0.2794449490805467 val loss: 0.3202557420439046\n",
      "Epoch 311 train loss: 0.2631603827906979 val loss: 0.33058302557986713\n",
      "Epoch 312 train loss: 0.26693718110521636 val loss: 0.3320602483075598\n",
      "Epoch 313 train loss: 0.2638768720130126 val loss: 0.3332636679482201\n",
      "Epoch 314 train loss: 0.2630597713092963 val loss: 0.33627177831595356\n",
      "Epoch 315 train loss: 0.26389848813414574 val loss: 0.33960387815275916\n",
      "Epoch 316 train loss: 0.2684213666452302 val loss: 0.33054864544259466\n",
      "Epoch 317 train loss: 0.2885240448845757 val loss: 0.3231331153732279\n",
      "Epoch 318 train loss: 0.2730225464536084 val loss: 0.3329666729854501\n",
      "Epoch 319 train loss: 0.2789949561158816 val loss: 0.34317821280463884\n",
      "Epoch 320 train loss: 0.2658227767381403 val loss: 0.3261072580581126\n",
      "Epoch 321 train loss: 0.2654626821478208 val loss: 0.32795561347966606\n",
      "Epoch 322 train loss: 0.26527932915422653 val loss: 0.3272494662391103\n",
      "Epoch 323 train loss: 0.2682178609073162 val loss: 0.33966165845808777\n",
      "Epoch 324 train loss: 0.2597527363234096 val loss: 0.3279744849904724\n",
      "Epoch 325 train loss: 0.2604438935716947 val loss: 0.33519008018724294\n",
      "Epoch 326 train loss: 0.2615743055111832 val loss: 0.33991206137706403\n",
      "Epoch 327 train loss: 0.2734209828078747 val loss: 0.33561597934559634\n",
      "Epoch 328 train loss: 0.27419833466410637 val loss: 0.33094511553645134\n",
      "Epoch 329 train loss: 0.2707902474535836 val loss: 0.3303132810346458\n",
      "Epoch 330 train loss: 0.2658760233885712 val loss: 0.33546762006438297\n",
      "Epoch 331 train loss: 0.2693867231408755 val loss: 0.32547572870617325\n",
      "Epoch 332 train loss: 0.28752146123184097 val loss: 0.3453713712161002\n",
      "Epoch 333 train loss: 0.2807361521654659 val loss: 0.32585863315540814\n",
      "Epoch 334 train loss: 0.27473990784751046 val loss: 0.34271793755824154\n",
      "Epoch 335 train loss: 0.27030519048372903 val loss: 0.3355832699200381\n",
      "Epoch 336 train loss: 0.2652628122932381 val loss: 0.33524005487561226\n",
      "Epoch 337 train loss: 0.26275976730717554 val loss: 0.33128160578401195\n",
      "Epoch 338 train loss: 0.2647736054327753 val loss: 0.35208611943475576\n",
      "Epoch 339 train loss: 0.26072931844327185 val loss: 0.3343461695248666\n",
      "Epoch 340 train loss: 0.2720641176733706 val loss: 0.3225215960132039\n",
      "Epoch 341 train loss: 0.27530133003989854 val loss: 0.34276295396620815\n",
      "Epoch 342 train loss: 0.2628475526140796 val loss: 0.3326001892919126\n",
      "Epoch 343 train loss: 0.2702517213920752 val loss: 0.3568874344067729\n",
      "Epoch 344 train loss: 0.26427954443626933 val loss: 0.33481503820613673\n",
      "Epoch 345 train loss: 0.2641885575320986 val loss: 0.33483124038447504\n",
      "Epoch 346 train loss: 0.26218272422750794 val loss: 0.33175442497367447\n",
      "Epoch 347 train loss: 0.2626961201429367 val loss: 0.33509785082677135\n",
      "Epoch 348 train loss: 0.2575681906607416 val loss: 0.33461652914791\n",
      "Epoch 349 train loss: 0.2611971065402031 val loss: 0.33805520379025006\n",
      "Epoch 350 train loss: 0.2577981601158778 val loss: 0.33927143753870675\n",
      "Epoch 351 train loss: 0.2538798683219486 val loss: 0.33078414468985534\n",
      "Epoch 352 train loss: 0.25552574743827183 val loss: 0.338328803686992\n",
      "Epoch 353 train loss: 0.25560143035319116 val loss: 0.3427090816523718\n",
      "Epoch 354 train loss: 0.2529183328151703 val loss: 0.3322853402275106\n",
      "Epoch 355 train loss: 0.2539661111931006 val loss: 0.3387587891648645\n",
      "Epoch 356 train loss: 0.25404929088221656 val loss: 0.34396923738329305\n",
      "Epoch 357 train loss: 0.2719944265153673 val loss: 0.33346542950881564\n",
      "Epoch 358 train loss: 0.268770346376631 val loss: 0.3396249832990377\n",
      "Epoch 359 train loss: 0.28479876609312166 val loss: 0.3277873325607051\n",
      "Epoch 360 train loss: 0.2704844312535392 val loss: 0.3412614063076351\n",
      "Epoch 361 train loss: 0.26530530411336156 val loss: 0.3286568550797908\n",
      "Epoch 362 train loss: 0.2615073100560241 val loss: 0.3318476468810569\n",
      "Epoch 363 train loss: 0.2825933464699321 val loss: 0.33281708047117875\n",
      "Epoch 364 train loss: 0.2739879373047087 val loss: 0.3270299364205288\n",
      "Epoch 365 train loss: 0.2652204158405463 val loss: 0.33894526019044546\n",
      "Epoch 366 train loss: 0.26973934628897245 val loss: 0.34712615495790605\n",
      "Epoch 367 train loss: 0.29605379982127084 val loss: 0.35609989775263745\n",
      "Epoch 368 train loss: 0.2711140978667471 val loss: 0.32829050379602803\n",
      "Epoch 369 train loss: 0.27216179594397544 val loss: 0.33255999519125273\n",
      "Epoch 370 train loss: 0.2832488466468122 val loss: 0.3398579918657956\n",
      "Epoch 371 train loss: 0.26385125923487873 val loss: 0.32144813919844833\n",
      "Epoch 372 train loss: 0.263124903705385 val loss: 0.33366210013628006\n",
      "Epoch 373 train loss: 0.2550874541203181 val loss: 0.3352808545788993\n",
      "Epoch 374 train loss: 0.2571251923011409 val loss: 0.3277713535758464\n",
      "Epoch 375 train loss: 0.257707433650891 val loss: 0.3270648454680391\n",
      "Epoch 376 train loss: 0.25642987704939313 val loss: 0.32926621992626914\n",
      "Epoch 377 train loss: 0.2517410751018259 val loss: 0.3294016185985959\n",
      "Epoch 378 train loss: 0.24988313871953222 val loss: 0.3295029979199171\n",
      "Epoch 379 train loss: 0.24996769362025792 val loss: 0.34232107703776465\n",
      "Epoch 380 train loss: 0.24941043249434894 val loss: 0.34114607293968613\n",
      "Epoch 381 train loss: 0.24996021100216442 val loss: 0.3405585440442614\n",
      "Epoch 382 train loss: 0.25531272242466607 val loss: 0.3345300137348797\n",
      "Epoch 383 train loss: 0.26152690773208936 val loss: 0.34217806897409586\n",
      "Epoch 384 train loss: 0.26614499530858465 val loss: 0.34188607084038464\n",
      "Epoch 385 train loss: 0.2760343372821808 val loss: 0.33012843447858875\n",
      "Epoch 386 train loss: 0.2683016492260827 val loss: 0.3363076429004255\n",
      "Epoch 387 train loss: 0.26229501167933145 val loss: 0.3487816883817963\n",
      "Epoch 388 train loss: 0.25221144548720786 val loss: 0.3330527152866125\n",
      "Epoch 389 train loss: 0.2510511888398064 val loss: 0.3369272324540045\n",
      "Epoch 390 train loss: 0.26691365126106475 val loss: 0.35554721967681596\n",
      "Epoch 391 train loss: 0.25539858523342346 val loss: 0.3333762746466243\n",
      "Epoch 392 train loss: 0.2532895342343383 val loss: 0.34104073833188286\n",
      "Epoch 393 train loss: 0.25197719832261406 val loss: 0.3424888968305743\n",
      "Epoch 394 train loss: 0.24870639418562254 val loss: 0.33549851494962757\n",
      "Epoch 395 train loss: 0.24826515383190578 val loss: 0.33006303101454093\n",
      "Epoch 396 train loss: 0.2533694968455368 val loss: 0.33277042720304884\n",
      "Epoch 397 train loss: 0.25055023663573794 val loss: 0.34050237565584807\n",
      "Epoch 398 train loss: 0.24736768611603313 val loss: 0.3439343205612639\n",
      "Epoch 399 train loss: 0.24476973803506957 val loss: 0.33971053086545155\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from modulus.launch.utils import save_checkpoint, load_checkpoint\n",
    "checkpoint_path = 'checkpoints'\n",
    "os.makedirs(checkpoint_path,exist_ok=True)\n",
    "epoch_init = 0 #load_checkpoint(checkpoint_path,model,optimizer,scheduler,scaler,device=device)\n",
    "num_epochs = 400\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def compute_average_loss(dataloader):\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            loss = compute_loss(batch)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "for epoch in range(epoch_init,num_epochs):\n",
    "    train_set_loss = compute_average_loss(dataloader)\n",
    "    val_set_loss = compute_average_loss(val_dataloader)\n",
    "    print(f'Epoch {epoch} train loss: {train_set_loss} val loss: {val_set_loss}')\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        loss = compute_loss(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "    scheduler.step()\n",
    "    if epoch % 10 == 0:\n",
    "        save_checkpoint(checkpoint_path,model,optimizer,scheduler,scaler,epoch)\n",
    "save_checkpoint(checkpoint_path,model,optimizer,scheduler,scaler,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50, 100)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataloader), len(dataloader), len(ds_dgl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f6941058194f31bf07ee301cabb25c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:44915/index.html?ui=P_0x7a28660cc8d0_1&reconnect=auto\" class=\"pyvi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.to(device)\n",
    "g=ds_dgl[0].to(device)\n",
    "g_pred = g.clone().to(device)\n",
    "ndx, edx = get_node_edge_X(g)\n",
    "with torch.no_grad():\n",
    "    y_pred = model(ndx,edx,g)\n",
    "set_graph_features(g_pred, ndx, edx, y_pred)\n",
    "ds_dgl.plot_surface(g_pred,\"Pressure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7843e8ee992457ab420f89cb19974a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:44915/index.html?ui=P_0x7a28b3f0f310_2&reconnect=auto\" class=\"pyvi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_dgl.plot_surface(g,\"Pressure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.6068e-10, -6.7290e-02,  0.0000e+00], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dgl.compute_aggregate_force(g, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0038e-04, -3.2547e-02, -5.1706e-05], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dgl.compute_aggregate_force(g_pred,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
